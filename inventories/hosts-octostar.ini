[rke2_cluster:children]
rke2_servers
rke2_agents

[rke2_cluster:vars]
# Basic cluster configuration
# IMPORTANT: Generate a secure random token before deployment
# Use: openssl rand -hex 32 | head -c 48
rke2_token=a3f8b9c2d1e4a7b8c9d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d2e3f4a5b6
rke2_api_ip=10.14.100.1
# Kubernetes version for kubectl download (must match RKE2's Kubernetes version)
# This is used by playbook.yml for downloading the correct kubectl binary
kubernetes_version=v1.32.7

# Administrative user for Kubernetes access (must be created manually before deployment)
# This user will have kubectl, helm, and k9s access configured
cluster_admin_user=adm4n

# SQLite Mode Configuration (recommended for LXC deployments)
# Use SQLite instead of etcd for better LXC container compatibility
# NOTE: SQLite mode supports single-server + multiple agents architecture
rke2_use_sqlite=true

# Container Runtime Configuration
# Use RKE2's embedded containerd (Docker-free architecture)
rke2_use_docker=false
rke2_container_runtime=containerd

# LXC Container Optimizations
# Extended timeouts for LXC container environments
rke2_install_timeout=600
rke2_server_start_timeout=600
rke2_agent_start_timeout=600

# GPU Support Configuration (for nodes with GPUs)
# Uses NVIDIA Container Toolkit with RKE2's embedded containerd
install_nvidia_container_toolkit=true
gpu_nodes_enabled=true

# Compatibility variable for the role
k8s_cluster=rke2_cluster

# Network configuration for VLAN 14
rke2_cluster_cidr=['10.42.0.0/16']
rke2_service_cidr=['10.43.0.0/16']
rke2_cluster_dns=10.43.0.10

# Configurable naming (maintains backward compatibility)
node_name_prefix=os-env-
# GPU node detection: either hostname contains this pattern OR set is_gpu_node=true on individual hosts
gpu_node_pattern=gpu

# The file where to store the Kubernetes client configuration
rke2_download_kubeconf_file_name=hosts.yaml

# Configure the local-path-provisioner StorageClass
use_local_path_provisioner=true

# Storage configuration
local_path_provisioner_path=/mnt/data

# Rancher UI installation and configuration
install_rancher=true
rancher_hostname=rancher.octostar.lan
rancher_bootstrap_password=admin123

[rke2_servers]
# Single control plane server (SQLite mode requirement)
# SQLite datastore does not support HA control plane
10.14.100.1 ansible_user=adm4n rke2_type=server

[rke2_agents]
# Worker nodes can be multiple in SQLite mode
# GPU nodes automatically detected by gpu_node_pattern or is_gpu_node variable
10.14.100.2 ansible_user=adm4n rke2_type=agent is_gpu_node=false
10.14.100.3 ansible_user=adm4n rke2_type=agent is_gpu_node=true

# Optional: GPU nodes group for easy identification
[gpu_nodes]
10.14.100.3 ansible_user=adm4n rke2_type=agent is_gpu_node=true

[all:vars]
# SSH and privilege escalation configuration for passwordless operation
ansible_ssh_private_key_file=~/.ssh/id_ed25519
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
ansible_become=yes
ansible_become_method=sudo
ansible_become_user=root
ansible_become_pass=