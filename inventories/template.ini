# =============================================================================
# RKE2 CLUSTER INVENTORY TEMPLATE
# =============================================================================
#
# INSTRUCTIONS:
# 1. Copy this file to your desired inventory name (e.g., 'hosts-mylab.ini')
# 2. Replace ALL placeholders marked with <...> with your actual values
# 3. Update the IP addresses in the [rke2_servers] and [rke2_agents] sections
# 4. Generate a secure rke2_token using: openssl rand -hex 32 | head -c 48
# 5. Configure your domain and passwords
# 6. Run deployment with your inventory file
#
# SECURITY WARNING: 
# - Change ALL passwords and tokens to secure random values
# - Use SSH keys for passwordless authentication
# - Keep this file secure and never commit credentials to git
#
# =============================================================================

[rke2_cluster:children]
rke2_servers
rke2_agents

[rke2_cluster:vars]
# =============================================================================
# BASIC CLUSTER CONFIGURATION
# =============================================================================

# CRITICAL: Generate a secure random token before deployment
# Use: openssl rand -hex 32 | head -c 48
# Example: a3f8b9c2d1e4a7b8c9d2e3f4a5b6c7d8e9f0a1b2c3d4e5f6a7b8c9d2e3f4a5b6
rke2_token=<GENERATE-SECURE-TOKEN-HERE>

# IP address of your primary control plane server
rke2_api_ip=<CONTROL-PLANE-IP>

# Kubernetes version for kubectl download (must match RKE2's Kubernetes version)
# This is used by playbook.yml for downloading the correct kubectl binary
kubernetes_version=v1.32.7

# =============================================================================
# USER CONFIGURATION
# =============================================================================

# Administrative user for Kubernetes access (must be created manually before deployment)
# This user will have kubectl, helm, and k9s access configured
# Should be the same user you use to SSH into the nodes
cluster_admin_user=<YOUR-ADMIN-USERNAME>

# =============================================================================
# STORAGE MODE CONFIGURATION
# =============================================================================

# SQLite Mode Configuration (RECOMMENDED for LXC deployments)
# Use SQLite instead of etcd for better LXC container compatibility
# NOTE: SQLite mode supports single-server + multiple agents architecture
# Set to true for LXC containers, false for VMs with HA control plane
rke2_use_sqlite=true

# =============================================================================
# CONTAINER RUNTIME CONFIGURATION
# =============================================================================

# Use RKE2's embedded containerd (Docker-free architecture)
# Recommended for modern Kubernetes deployments
rke2_use_docker=false
rke2_container_runtime=containerd

# =============================================================================
# LXC CONTAINER OPTIMIZATIONS
# =============================================================================

# Extended timeouts for LXC container environments
# Increase these if running on slower hardware or networks
rke2_install_timeout=600
rke2_server_start_timeout=600
rke2_agent_start_timeout=600

# =============================================================================
# GPU SUPPORT CONFIGURATION
# =============================================================================

# GPU Support Configuration (for nodes with GPUs)
# Uses NVIDIA Container Toolkit with RKE2's embedded containerd
install_nvidia_container_toolkit=true
gpu_nodes_enabled=true

# Compatibility variable for the ansible-role-rke2
k8s_cluster=rke2_cluster

# =============================================================================
# NETWORK CONFIGURATION
# =============================================================================

# Network configuration - adjust if conflicts with your existing networks
# Default ranges work for most environments
rke2_cluster_cidr=['10.42.0.0/16']
rke2_service_cidr=['10.43.0.0/16']
rke2_cluster_dns=10.43.0.10

# =============================================================================
# NAMING AND IDENTIFICATION
# =============================================================================

# Configurable naming (maintains backward compatibility)
# Customize this prefix for your environment
node_name_prefix=<YOUR-CLUSTER-NAME>-rke2-

# GPU node detection: either hostname contains this pattern OR set is_gpu_node=true on individual hosts
gpu_node_pattern=gpu

# The file where to store the Kubernetes client configuration
# This will be saved in the kubeconfig/ directory
rke2_download_kubeconf_file_name=<YOUR-CLUSTER-NAME>.yaml

# =============================================================================
# STORAGE CONFIGURATION
# =============================================================================

# Configure the local-path-provisioner StorageClass
# This provides persistent storage for your applications
use_local_path_provisioner=true

# Storage configuration - where persistent volumes will be stored
# Ensure this path exists and has sufficient space
local_path_provisioner_path=/mnt/data

# =============================================================================
# RANCHER UI CONFIGURATION
# =============================================================================

# Rancher UI installation and configuration
# Set to false if you don't want the Rancher management interface
install_rancher=true

# Replace with your actual domain name
rancher_hostname=rancher.<YOUR-DOMAIN>

# Generate a secure password for Rancher bootstrap
# This is the initial admin password for the Rancher UI
rancher_bootstrap_password=<GENERATE-SECURE-PASSWORD>

# =============================================================================
# NODE DEFINITIONS - UPDATE WITH YOUR ACTUAL CONFIGURATION
# =============================================================================

[rke2_servers]
# Single control plane server (SQLite mode requirement)
# SQLite datastore does not support HA control plane
# Format: IP_ADDRESS ansible_user=USERNAME rke2_type=server
#
# Example:
# 10.14.100.1 ansible_user=adm8n rke2_type=server
<CONTROL-PLANE-IP> ansible_user=<USERNAME> rke2_type=server

[rke2_agents]
# Worker nodes can be multiple in SQLite mode
# GPU nodes automatically detected by gpu_node_pattern or is_gpu_node variable
# Format: IP_ADDRESS ansible_user=USERNAME rke2_type=agent [is_gpu_node=true]
#
# Examples:
# 10.14.100.2 ansible_user=adm8n rke2_type=agent
# 10.14.100.3 ansible_user=adm8n rke2_type=agent is_gpu_node=true
<WORKER-1-IP> ansible_user=<USERNAME> rke2_type=agent
<WORKER-2-IP> ansible_user=<USERNAME> rke2_type=agent is_gpu_node=true

# =============================================================================
# OPTIONAL: GPU NODES GROUP
# =============================================================================
# Optional: GPU nodes group for easy identification and management
# Uncomment and configure if you have dedicated GPU nodes
#
# [gpu_nodes]
# <GPU-NODE-IP> ansible_user=<USERNAME> rke2_type=agent is_gpu_node=true

# =============================================================================
# ANSIBLE CONNECTION CONFIGURATION - CUSTOMIZE FOR YOUR ENVIRONMENT
# =============================================================================

[all:vars]
# SSH and privilege escalation configuration for passwordless operation
# Update the SSH key path to match your actual key location
ansible_ssh_private_key_file=~/.ssh/id_ed25519
ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null'
ansible_become=yes
ansible_become_method=sudo
ansible_become_user=root
ansible_become_pass=

# =============================================================================
# DEPLOYMENT CHECKLIST
# =============================================================================
#
# Before running the deployment, ensure:
# 
# □ All IP addresses are updated to match your environment
# □ rke2_token is generated using: openssl rand -hex 32 | head -c 48
# □ rancher_bootstrap_password is set to a secure value
# □ cluster_admin_user matches the user you created on all nodes
# □ SSH key path (ansible_ssh_private_key_file) is correct
# □ All nodes are accessible via SSH with the specified user
# □ All nodes have sudo/root access configured
# □ Domain name for Rancher is configured in your DNS
# □ GPU nodes have NVIDIA hardware if is_gpu_node=true is set
# □ Storage path (/mnt/data) exists on all nodes with sufficient space
#
# DEPLOYMENT COMMANDS:
# 1. Trust SSH hosts: ./trust_ssh_hosts.sh <your-inventory-file>
# 2. Deploy cluster: ansible-playbook -i <your-inventory-file> playbooks/playbook.yml
# 3. Install tools: ansible-playbook -i <your-inventory-file> playbooks/post_playbook_tools.yml
# 4. Setup helm repos: ansible-playbook -i <your-inventory-file> playbooks/post_playbook_helm_repos.yml
#
# =============================================================================