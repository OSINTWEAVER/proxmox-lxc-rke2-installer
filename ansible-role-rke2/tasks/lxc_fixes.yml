---
# LXC-specific RKE2 configuration - simplified for LXC-only deployment

# Configure AppArmor for LXC container compatibility
- name: Check AppArmor status
  ansible.builtin.shell: |
    if command -v aa-status >/dev/null 2>&1; then
      echo "AppArmor tools available"
      aa-status 2>/dev/null || echo "AppArmor may be restricted in LXC"
    else
      echo "AppArmor tools not found (normal in LXC)"
    fi
  register: apparmor_status
  changed_when: false

- name: Display AppArmor status
  ansible.builtin.debug:
    var: apparmor_status.stdout_lines

- name: Create kubelet systemd override directory
  ansible.builtin.file:
    path: /etc/systemd/system/kubelet.service.d
    state: directory
    owner: root
    group: root
    mode: '0755'

- name: Configure kubelet to disable AppArmor feature gate
  ansible.builtin.copy:
    content: |
      [Service]
      Environment="KUBELET_EXTRA_ARGS=--feature-gates=AppArmor=false"
    dest: /etc/systemd/system/kubelet.service.d/10-apparmor-override.conf
    owner: root
    group: root
    mode: '0644'
  notify: "Reload systemd daemon"

# Create kernel parameter workarounds for LXC containers
- name: Create kernel parameter workarounds
  ansible.builtin.shell: |
    mkdir -p /var/lib/rancher/rke2/agent/proc-sys-stubs
    
    for param in vm/overcommit_memory kernel/panic kernel/panic_on_oops; do
      param_path="/proc/sys/$param"
      if [ -f "$param_path" ]; then
        if ! echo "test" > "$param_path" 2>/dev/null; then
          stub_path="/var/lib/rancher/rke2/agent/proc-sys-stubs/$(echo $param | tr '/' '_')"
          cat "$param_path" > "$stub_path" 2>/dev/null || echo "1" > "$stub_path"
          chmod 666 "$stub_path"
          mount --bind "$stub_path" "$param_path" 2>/dev/null || true
        fi
      fi
    done
  register: kernel_param_check
  changed_when: false

- name: Create systemd service for LXC kernel parameter persistence
  ansible.builtin.copy:
    dest: /etc/systemd/system/lxc-kernel-params.service
    content: |
      [Unit]
      Description=LXC Kernel Parameter Workarounds
      Before=rke2-server.service
      After=local-fs.target

      [Service]
      Type=oneshot
      RemainAfterExit=yes
      ExecStart=/bin/bash -c 'for param in vm/overcommit_memory kernel/panic kernel/panic_on_oops; do stub_path="/var/lib/rancher/rke2/agent/proc-sys-stubs/$(echo $param | tr "/" "_")"; param_path="/proc/sys/$param"; if [ -f "$stub_path" ] && [ -f "$param_path" ]; then mount --bind "$stub_path" "$param_path" 2>/dev/null || true; fi; done'

      [Install]
      WantedBy=multi-user.target
    mode: '0644'

- name: Enable and start LXC kernel parameter service
  ansible.builtin.systemd:
    name: lxc-kernel-params.service
    enabled: yes
    state: started
    daemon_reload: yes

- name: Display kernel parameter check results
  ansible.builtin.debug:
    var: kernel_param_check.stdout_lines

# Check and prepare critical kernel modules for RKE2
- name: Check critical kernel modules
  ansible.builtin.shell: |
    CRITICAL_MODULES="overlay"
    OPTIONAL_MODULES="br_netfilter"
    
    missing_critical=""
    
    for module in $CRITICAL_MODULES; do
      if ! lsmod | grep -q "^$module "; then
        missing_critical="$missing_critical $module"
      fi
    done
    
    if [ -n "$missing_critical" ]; then
      echo "ERROR: Critical modules missing:$missing_critical"
      echo "These must be loaded on the Proxmox host with 'modprobe'"
      exit 1
    fi
    
    echo "SUCCESS: All critical kernel modules available"
  register: kernel_module_check
  failed_when: kernel_module_check.rc != 0

- name: Display kernel module check results
  ansible.builtin.debug:
    var: kernel_module_check.stdout_lines

# Validate LXC container configuration prerequisites
- name: Validate LXC container configuration
  ansible.builtin.shell: |
    echo "LXC Container Configuration Validation"
    echo "======================================="
    
    if [ -f /proc/1/status ]; then
      if grep -q "CapBnd.*0000003fffffffff" /proc/1/status; then
        echo "SUCCESS: Container is privileged"
      else
        echo "ERROR: Container appears to be unprivileged"
      fi
    fi
    
    echo ""
    echo "Kernel Module Accessibility:"
    for module in br_netfilter overlay ip_tables ip6_tables nf_nat xt_conntrack; do
      if [ -f /proc/modules ]; then
        if grep -q "^$module " /proc/modules; then
          echo "SUCCESS: $module: Available"
        else
          echo "WARNING: $module: Not loaded"
        fi
      else
        echo "ERROR: /proc/modules not accessible"
      fi
    done
    
    echo ""
    echo "Mount Point Accessibility:"
    if mount | grep -q "proc.*rw"; then
      echo "SUCCESS: /proc mounted read-write"
    else
      echo "ERROR: /proc not mounted read-write"
    fi
    
    if mount | grep -q "sys.*rw"; then
      echo "SUCCESS: /sys mounted read-write"  
    else
      echo "ERROR: /sys not mounted read-write"
    fi
    
    echo ""
    echo "Cgroup Accessibility:"
    if [ -d /sys/fs/cgroup ]; then
      echo "SUCCESS: /sys/fs/cgroup accessible"
      if [ -f /sys/fs/cgroup/cgroup.controllers ]; then
        echo "SUCCESS: Cgroup v2 detected"
        available_controllers=$(cat /sys/fs/cgroup/cgroup.controllers)
        echo "   Available controllers: $available_controllers"
      elif [ -d /sys/fs/cgroup/memory ]; then
        echo "SUCCESS: Cgroup v1 detected"
      fi
    else
      echo "ERROR: /sys/fs/cgroup not accessible"
    fi
  register: lxc_validation
  changed_when: false

- name: Display LXC validation results
  ansible.builtin.debug:
    var: lxc_validation.stdout_lines

# Essential kernel modules and networking validation
- name: Ensure critical kernel modules are available
  ansible.builtin.shell: |
    echo "Critical Kernel Modules for RKE2"
    echo "================================"
    
    REQUIRED_MODULES="br_netfilter overlay ip_tables ip6_tables nf_nat xt_conntrack nf_conntrack"
    
    for module in $REQUIRED_MODULES; do
      if [ -f /proc/modules ]; then
        if grep -q "^$module " /proc/modules; then
          echo "SUCCESS: $module: LOADED"
        else
          echo "ERROR: $module: NOT LOADED"
        fi
      else
        echo "WARNING: Cannot check $module"
      fi
    done
    
    echo ""
    echo "Bridge Netfilter Configuration:"
    if [ -f /proc/sys/net/bridge/bridge-nf-call-iptables ]; then
      bridge_iptables=$(cat /proc/sys/net/bridge/bridge-nf-call-iptables)
      if [ "$bridge_iptables" = "1" ]; then
        echo "SUCCESS: bridge-nf-call-iptables: ENABLED"
      else
        echo "ERROR: bridge-nf-call-iptables: DISABLED"
      fi
    else
      echo "ERROR: bridge-nf-call-iptables: NOT ACCESSIBLE"
    fi
    
    if [ -f /proc/sys/net/bridge/bridge-nf-call-ip6tables ]; then
      bridge_ip6tables=$(cat /proc/sys/net/bridge/bridge-nf-call-ip6tables)
      if [ "$bridge_ip6tables" = "1" ]; then
        echo "SUCCESS: bridge-nf-call-ip6tables: ENABLED"
      else
        echo "ERROR: bridge-nf-call-ip6tables: DISABLED"
      fi
    else
      echo "ERROR: bridge-nf-call-ip6tables: NOT ACCESSIBLE"
    fi
  register: kernel_module_validation
  changed_when: false

- name: Display kernel module validation
  ansible.builtin.debug:
    var: kernel_module_validation.stdout_lines

- name: Install LXC-compatible systemd service (only if not using SQLite mode)
  ansible.builtin.template:
    src: rke2-server-lxc.service.j2
    dest: /usr/local/lib/systemd/system/rke2-server.service
    owner: root
    group: root
    mode: 0644
  when: 
    - inventory_hostname in groups[rke2_servers_group_name]
    - not (rke2_use_sqlite | default(false))
  notify: "Reload systemd daemon"

- name: Install LXC-compatible agent systemd service (only if not using SQLite mode)
  ansible.builtin.template:
    src: rke2-agent-lxc.service.j2
    dest: /usr/local/lib/systemd/system/rke2-agent.service
    owner: root
    group: root
    mode: 0644
  when: 
    - inventory_hostname in groups[rke2_agents_group_name]
    - not (rke2_use_sqlite | default(false))
  notify: "Reload systemd daemon"

- name: Note about SQLite mode systemd override precedence
  ansible.builtin.debug:
    msg: |
      SQLite mode detected - using systemd service overrides instead of LXC service replacement.
      This provides better compatibility with the SQLite-specific optimizations.
  when: 
    - rke2_use_sqlite | default(false)

- name: Run kernel module check
  ansible.builtin.shell: |
    echo "=== Kernel Module Check ==="
    for module in br_netfilter overlay; do
      if [ -f /proc/modules ]; then
        if grep -q "^$module " /proc/modules; then
          echo "$module: LOADED"
        else
          echo "$module: NOT LOADED"
        fi
      else
        echo "$module: /proc/modules not available"
      fi
    done
  register: module_check_result
  failed_when: false
  changed_when: false

- name: Display module check results
  ansible.builtin.debug:
    var: module_check_result.stdout_lines

- name: Note about br_netfilter in LXC
  ansible.builtin.debug:
    msg: |
      INFO: br_netfilter may show as 'NOT LOADED' in LXC containers even when it's 
      working correctly on the Proxmox host. This is normal LXC behavior.
      
      Based on your Proxmox host diagnosis, br_netfilter is properly loaded.
      Continuing with RKE2 deployment...

# Enhanced sysctl configuration for LXC
- name: Create comprehensive LXC sysctl configuration
  ansible.builtin.copy:
    dest: /etc/sysctl.d/99-rke2-lxc.conf
    content: |
      # RKE2 LXC Container Optimizations
      
      # === NETWORKING ===
      net.bridge.bridge-nf-call-iptables = 1
      net.bridge.bridge-nf-call-ip6tables = 1
      net.bridge.bridge-nf-call-arptables = 1
      
      # IP forwarding
      net.ipv4.ip_forward = 1
      net.ipv4.conf.all.forwarding = 1
      net.ipv6.conf.all.forwarding = 1
      net.ipv4.conf.default.forwarding = 1
      net.ipv6.conf.default.forwarding = 1
      
      # Connection tracking optimizations
      net.netfilter.nf_conntrack_tcp_timeout_established = 86400
      net.netfilter.nf_conntrack_tcp_timeout_close_wait = 3600
      
      # === CONTAINER RUNTIME OPTIMIZATIONS ===
      net.core.somaxconn = 32768
      
      # Memory management for containers
      vm.max_map_count = 262144
      vm.overcommit_memory = 1
      
      # File system optimizations
      fs.inotify.max_user_instances = 8192
      fs.inotify.max_user_watches = 1048576
      fs.file-max = 1048576
      
      # Process limits
      kernel.pid_max = 4194304
      kernel.threads-max = 1048576
      
      # === SWAP CONFIGURATION ===
      vm.swappiness = 1
      
      # === PERFORMANCE TUNING ===
      net.ipv4.tcp_congestion_control = bbr
      net.ipv4.tcp_rmem = 4096 87380 134217728
      net.ipv4.tcp_wmem = 4096 65536 134217728
      
      net.ipv4.tcp_tw_reuse = 1
      net.ipv4.ip_local_port_range = 1024 65535
    owner: root
    group: root
    mode: 0644

- name: Apply LXC sysctl configuration with error tolerance
  ansible.builtin.command: sysctl -p /etc/sysctl.d/99-rke2-lxc.conf
  register: sysctl_result
  failed_when: false
  changed_when: sysctl_result.rc == 0

- name: Display sysctl results
  ansible.builtin.debug:
    msg: |
      Sysctl configuration applied:
      Return code: {{ sysctl_result.rc }}
      {% if sysctl_result.stdout %}
      Output: {{ sysctl_result.stdout }}
      {% endif %}
      {% if sysctl_result.stderr %}
      Warnings (normal in LXC): {{ sysctl_result.stderr }}
      {% endif %}

# Create /dev/kmsg for kubelet
- name: Check if /dev/kmsg exists
  ansible.builtin.stat:
    path: /dev/kmsg
  register: dev_kmsg_stat

- name: Create /dev/kmsg symlink for kubelet
  ansible.builtin.shell: |
    if [ ! -e /dev/kmsg ]; then
      ln -sf /dev/null /dev/kmsg
      echo "Created /dev/kmsg -> /dev/null"
    else
      echo "/dev/kmsg already exists"
    fi
  when: not dev_kmsg_stat.stat.exists

- name: Create systemd tmpfile for persistent /dev/kmsg
  ansible.builtin.copy:
    dest: /etc/tmpfiles.d/kmsg.conf
    content: |
      # Kubernetes kubelet requires /dev/kmsg
      L /dev/kmsg - - - - /dev/null
    owner: root
    group: root
    mode: 0644

- name: Create advanced /dev/kmsg service with proper dependencies
  ansible.builtin.copy:
    dest: /etc/systemd/system/create-kmsg.service
    content: |
      [Unit]
      Description=Create /dev/kmsg for LXC Kubernetes compatibility
      Documentation=https://kubernetes.io/docs/setup/production-environment/container-runtimes/
      DefaultDependencies=false
      Before=rke2-server.service rke2-agent.service kubelet.service
      ConditionPathExists=!/dev/kmsg

      [Service]
      Type=oneshot
      ExecStart=/bin/sh -c 'ln -sf /dev/null /dev/kmsg 2>/dev/null || true'
      ExecStart=/bin/sh -c 'systemd-tmpfiles --create /etc/tmpfiles.d/kmsg.conf || true'
      RemainAfterExit=yes
      StandardOutput=journal
      StandardError=journal

      [Install]
      WantedBy=multi-user.target sysinit.target
    owner: root
    group: root
    mode: 0644

- name: Enable and start /dev/kmsg creation service
  ansible.builtin.systemd:
    name: create-kmsg.service
    enabled: true
    state: started
    daemon_reload: true

# Ensure RKE2 configuration directory exists
- name: Create RKE2 configuration directory
  ansible.builtin.file:
    path: /etc/rancher/rke2
    state: directory
    owner: root
    group: root
    mode: '0755'
    recurse: true

# Configure containerd for cgroupfs instead of systemd
- name: Deploy LXC-optimized containerd configuration
  ansible.builtin.template:
    src: containerd-lxc.toml.j2
    dest: /etc/rancher/rke2/config.toml.tmpl
    owner: root
    group: root
    mode: '0644'
    backup: yes

- name: Create containerd config directory
  ansible.builtin.file:
    path: /var/lib/rancher/rke2/agent/etc/containerd
    state: directory
    owner: root
    group: root
    mode: '0755'
    recurse: true

- name: Copy containerd config to RKE2 agent location
  ansible.builtin.copy:
    src: /etc/rancher/rke2/config.toml.tmpl
    dest: /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl
    owner: root
    group: root
    mode: '0644'
    remote_src: yes

# Note: AppArmor disable is already included in the containerd-lxc.toml.j2 template
# No need for additional lineinfile modifications

# REMOVED: config-lxc-override.yaml creation - no longer needed
# All LXC optimizations are now included in the main kubelet-config.yaml template
# This prevents configuration conflicts and duplicate kubelet-arg entries

- name: Check if main config exists before merging
  ansible.builtin.stat:
    path: /etc/rancher/rke2/config.yaml
  register: main_config_stat

# Prevent kubelet from looking for config files but allow device-plugins directory
- name: Prevent kubelet config file issues
  ansible.builtin.shell: |
    echo "Configuring kubelet directory structure for LXC container"
    
    mkdir -p /var/lib/kubelet/device-plugins
    mkdir -p /var/lib/kubelet/pods
    mkdir -p /var/lib/kubelet/plugins
    mkdir -p /var/lib/kubelet/plugins_registry
    mkdir -p /var/lib/kubelet/checkpoints
    
    rm -f /var/lib/kubelet/config.yaml
    
    chown -R root:root /var/lib/kubelet
    chmod -R 755 /var/lib/kubelet
    chmod 755 /var/lib/kubelet/device-plugins
    
    echo "# LXC Container Kubelet Configuration" > /var/lib/kubelet/README-LXC
    echo "# This kubelet instance is configured via kubelet-config.yaml" >> /var/lib/kubelet/README-LXC
    echo "# located at /etc/rancher/rke2/kubelet-config.yaml" >> /var/lib/kubelet/README-LXC
    echo "# DO NOT create conflicting config files in this directory" >> /var/lib/kubelet/README-LXC
    
    ls -la /var/lib/kubelet/
    
    echo "Kubelet directory structure configured for LXC compatibility"
  register: kubelet_config_prevention
  changed_when: false

- name: Create kubelet config directory override
  ansible.builtin.file:
    path: /etc/systemd/system/rke2-server.service.d
    state: directory
    mode: 0755
    owner: root
    group: root
  when: inventory_hostname in groups[rke2_servers_group_name]

- name: Display kubelet config prevention results
  ansible.builtin.debug:
    var: kubelet_config_prevention.stdout_lines

- name: Check if main config exists before merging
  ansible.builtin.stat:
    path: /etc/rancher/rke2/config.yaml
  register: main_config_stat

- name: Backup original config if it exists
  ansible.builtin.copy:
    src: /etc/rancher/rke2/config.yaml
    dest: /etc/rancher/rke2/config.yaml.backup
    remote_src: true
  when: main_config_stat.stat.exists

- name: Check config file existence before setting permissions
  ansible.builtin.stat:
    path: /etc/rancher/rke2/config.yaml
  register: config_file_check

- name: Ensure correct permissions on final config
  ansible.builtin.file:
    path: /etc/rancher/rke2/config.yaml
    owner: root
    group: root
    mode: 0600
  when: config_file_check.stat.exists

- name: Reload systemd daemon for LXC service files
  ansible.builtin.systemd:
    daemon_reload: true

- name: Ensure RKE2 server service is prepared for LXC deployment
  ansible.builtin.debug:
    msg: |
      RKE2 service preparation for LXC:
      - Service start/stop will be managed by main deployment tasks
      - LXC optimizations are applied to systemd service files
      - Configuration conflicts have been prevented
  when: inventory_hostname in groups[rke2_servers_group_name]

# GPU passthrough and device access optimizations
- name: Detect GPU nodes for proper device access
  ansible.builtin.shell: |
    if find /dev -name "nvidia*" -type c 2>/dev/null | grep -q nvidia || command -v nvidia-smi >/dev/null 2>&1; then
      echo "GPU_DETECTED"
    else
      echo "NO_GPU_DETECTED"
    fi
  register: gpu_detection_result
  changed_when: false

- name: Set GPU detection fact
  ansible.builtin.set_fact:
    has_nvidia_devices: "{{ 'GPU_DETECTED' in gpu_detection_result.stdout }}"

- name: Configure GPU device access for LXC containers
  ansible.builtin.shell: |
    echo "GPU PASSTHROUGH CONFIGURATION FOR LXC"
    echo "======================================"
    
    if [ -d /dev/nvidia-smi ] || [ -f /usr/bin/nvidia-smi ]; then
      echo "SUCCESS: NVIDIA devices detected"
      echo "NOTE: NVIDIA Container Toolkit will be installed directly for LXC compatibility"
      
      nvidia_devices=$(find /dev -name "nvidia*" 2>/dev/null | wc -l)
      echo "INFO: Found $nvidia_devices NVIDIA device nodes"
      
    else
      echo "INFO: No NVIDIA devices detected (this is normal for CPU-only deployments)"
    fi
    
    if [ -d /dev/dri ]; then
      dri_devices=$(ls -la /dev/dri/ | grep -c "card\|render")
      echo "INFO: Found $dri_devices DRI devices in /dev/dri"
    fi
    
    cat > /usr/local/bin/setup-gpu-devices.sh << 'EOFGPU'
    #!/bin/bash
    set -e
    
    if [ -d /dev/nvidia0 ]; then
        chmod 666 /dev/nvidia*
        echo "SUCCESS: NVIDIA device permissions set"
    fi
    
    if [ -d /dev/dri ]; then
        chmod 666 /dev/dri/*
        echo "SUCCESS: DRI device permissions set"
    fi
    EOFGPU
    
    chmod +x /usr/local/bin/setup-gpu-devices.sh
    echo "SUCCESS: GPU device setup script created"
  register: gpu_setup_result
  changed_when: false

# Install NVIDIA Container Toolkit for LXC environments
# GPU Operator has limitations in LXC, so we install toolkit directly
# Only run on nodes explicitly marked as GPU nodes
- name: Install NVIDIA Container Toolkit for GPU nodes
  ansible.builtin.shell: |
    echo "INSTALLING NVIDIA CONTAINER TOOLKIT FOR LXC"
    
    # Add NVIDIA Container Toolkit repository
    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
    curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
      sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
      tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
    
    # Update package list and install
    apt-get update
    apt-get install -y nvidia-container-toolkit
    
    # Verify nvidia-container-runtime is available
    if command -v nvidia-container-runtime >/dev/null 2>&1; then
      echo "SUCCESS: nvidia-container-runtime is available"
      echo "NOTE: containerd configuration includes nvidia runtime for LXC compatibility"
    else
      echo "WARNING: nvidia-container-runtime not found after installation"
    fi
    
    echo "SUCCESS: NVIDIA Container Toolkit installed for LXC"
  register: nvidia_toolkit_install
  changed_when: "'SUCCESS' in nvidia_toolkit_install.stdout"
  when: >
    (inventory_hostname in groups.get('gpu_nodes', [])) or 
    (is_gpu_node | default(false)) or
    (ansible_hostname | regex_search(gpu_node_pattern | default('gpu')))

- name: Display GPU setup results
  ansible.builtin.debug:
    var: gpu_setup_result.stdout_lines

- name: Display NVIDIA Container Toolkit installation results
  ansible.builtin.debug:
    var: nvidia_toolkit_install.stdout_lines
  when: nvidia_toolkit_install is defined

# Advanced LXC mount optimizations for Kubernetes workloads
- name: Configure advanced mount optimizations for LXC
  ansible.builtin.copy:
    dest: /usr/local/bin/lxc-mount-optimizations.sh
    content: |
      #!/bin/bash
      set -e
      
      echo "LXC MOUNT OPTIMIZATIONS"
      echo "======================="
      
      if ! mount | grep -q "proc.*rw"; then
        echo "WARNING: /proc not mounted read-write - this may cause issues"
      else
        echo "SUCCESS: /proc mounted read-write"
      fi
      
      if ! mount | grep -q "sys.*rw"; then
        echo "WARNING: /sys not mounted read-write - this may cause issues"
      else
        echo "SUCCESS: /sys mounted read-write"
      fi
      
      if ! findmnt / -o PROPAGATION | grep -q shared; then
        echo "INFO: Setting up shared mount propagation"
        mount --make-rshared /
        echo "SUCCESS: Root filesystem set to shared propagation"
      else
        echo "SUCCESS: Mount propagation already configured"
      fi
      
      if [ ! -d /run/k3s ]; then
        mkdir -p /run/k3s
      fi
      
      mkdir -p /var/lib/kubelet
      mkdir -p /var/lib/rancher/rke2
      mkdir -p /var/log/pods
      
      if command -v selinuxenabled >/dev/null 2>&1 && selinuxenabled; then
        echo "INFO: Configuring SELinux contexts for Kubernetes"
        setsebool -P container_manage_cgroup true
        setsebool -P virt_use_nfs true
      fi
      
      echo "SUCCESS: Mount optimizations complete"
    mode: 0755
    owner: root
    group: root

- name: Create systemd service for LXC mount optimizations
  ansible.builtin.copy:
    dest: /etc/systemd/system/lxc-mount-optimizations.service
    content: |
      [Unit]
      Description=LXC Mount Optimizations for Kubernetes
      Documentation=https://kubernetes.io/docs/setup/production-environment/
      DefaultDependencies=false
      Before=rke2-server.service rke2-agent.service containerd.service
      After=local-fs.target

      [Service]
      Type=oneshot
      ExecStart=/usr/local/bin/lxc-mount-optimizations.sh
      RemainAfterExit=yes
      StandardOutput=journal
      StandardError=journal

      [Install]
      WantedBy=multi-user.target
    owner: root
    group: root
    mode: 0644

- name: Enable LXC mount optimizations service
  ansible.builtin.systemd:
    name: lxc-mount-optimizations.service
    enabled: true
    state: started
    daemon_reload: true

# NOTE: Previous RKE2 v1.30.14+rke2r2 kube-scheduler bind-timeout bug fix removed
# as this issue is resolved in Kubernetes 1.33 and RKE2 v1.33.3+rke2r1

# Production-grade health check script with comprehensive validation
- name: Create comprehensive LXC health check script
  ansible.builtin.copy:
    dest: /usr/local/bin/rke2-lxc-health-check.sh
    content: |
      #!/bin/bash
      set -e
      
      echo "COMPREHENSIVE RKE2 LXC HEALTH CHECK"
      echo "==================================="
      
      RED='\033[0;31m'
      GREEN='\033[0;32m'
      YELLOW='\033[1;33m'
      BLUE='\033[0;34m'
      NC='\033[0m'
      
      echo -e "${BLUE}Container Environment:${NC}"
      if grep -qa container=lxc /proc/1/environ; then
        echo -e "  ${GREEN}SUCCESS: Running in LXC container${NC}"
      else
        echo -e "  ${YELLOW}WARNING: Not detected as LXC container${NC}"
      fi
      
      echo -e "${BLUE}Kernel Modules:${NC}"
      for module in br_netfilter overlay ip_tables ip6_tables nf_nat; do
        if grep -q "^$module " /proc/modules 2>/dev/null; then
          echo -e "  ${GREEN}SUCCESS: $module: loaded${NC}"
        else
          echo -e "  ${YELLOW}WARNING: $module: not loaded${NC}"
        fi
      done
      
      echo -e "${BLUE}Device Nodes:${NC}"
      if [ -e /dev/kmsg ]; then
        echo -e "  ${GREEN}SUCCESS: /dev/kmsg: exists${NC}"
      else
        echo -e "  ${RED}ERROR: /dev/kmsg: missing${NC}"
      fi
      
      echo -e "${BLUE}Sysctl Configuration:${NC}"
      if [ -f /proc/sys/net/bridge/bridge-nf-call-iptables ]; then
        bridge_iptables=$(cat /proc/sys/net/bridge/bridge-nf-call-iptables)
        if [ "$bridge_iptables" = "1" ]; then
          echo -e "  ${GREEN}SUCCESS: bridge-nf-call-iptables: enabled${NC}"
        else
          echo -e "  ${RED}ERROR: bridge-nf-call-iptables: disabled${NC}"
        fi
      else
        echo -e "  ${YELLOW}WARNING: bridge-nf-call-iptables: not accessible${NC}"
      fi
      
      echo -e "${BLUE}RKE2 Service Status:${NC}"
      if systemctl is-active rke2-server.service >/dev/null 2>&1; then
        echo -e "  ${GREEN}SUCCESS: RKE2 server service: active${NC}"
        
        if pgrep -f "rke2 server" >/dev/null; then
          echo -e "  ${GREEN}SUCCESS: RKE2 server process: running${NC}"
        else
          echo -e "  ${YELLOW}WARNING: RKE2 server process: not found${NC}"
        fi
      elif systemctl is_active rke2-agent.service >/dev/null 2>&1; then
        echo -e "  ${GREEN}SUCCESS: RKE2 agent service: active${NC}"
      else
        echo -e "  ${RED}ERROR: RKE2 service: not active${NC}"
        systemctl status rke2-server.service --no-pager -l || systemctl status rke2-agent.service --no-pager -l || true
      fi
      
      echo -e "${BLUE}Kubernetes API:${NC}"
      if [ -f /etc/rancher/rke2/rke2.yaml ]; then
        export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        PATH="/var/lib/rancher/rke2/bin:$PATH"
        
        if timeout 30 kubectl get nodes >/dev/null 2>&1; then
          echo -e "  ${GREEN}SUCCESS: Kubernetes API: responding${NC}"
          echo -e "${BLUE}Cluster Status:${NC}"
          kubectl get nodes -o wide | sed 's/^/    /'
          
          echo -e "${BLUE}System Pods:${NC}"
          kubectl get pods -n kube-system --no-headers | awk '{print "    " $1 ": " $3}' || true
          
        else
          echo -e "  ${YELLOW}INFO: Kubernetes API: not ready yet${NC}"
        fi
      else
        echo -e "  ${YELLOW}WARNING: RKE2 kubeconfig not found${NC}"
      fi
      
      echo -e "${BLUE}Container Runtime:${NC}"
      if [ -S /run/k3s/containerd/containerd.sock ]; then
        echo -e "  ${GREEN}SUCCESS: Containerd socket: available${NC}"
        
        if systemctl is-active containerd >/dev/null 2>&1; then
          echo -e "  ${GREEN}SUCCESS: Containerd service: active${NC}"
        fi
      else
        echo -e "  ${RED}ERROR: Containerd socket: not found${NC}"
      fi
      
      echo -e "${BLUE}Resource Usage:${NC}"
      memory_total=$(free -h | awk '/^Mem:/ {print $2}')
      memory_used=$(free -h | awk '/^Mem:/ {print $3}')
      disk_usage=$(df -h / | awk 'NR==2 {print $5}')
      
      echo -e "  Memory: $memory_used / $memory_total"
      echo -e "  Disk: $disk_usage used"
      
      echo -e "${BLUE}GPU Devices:${NC}"
      if [ -d /dev/nvidia0 ] || command -v nvidia-smi >/dev/null 2>&1; then
        echo -e "  ${GREEN}SUCCESS: NVIDIA devices detected${NC}"
        if command -v nvidia-smi >/dev/null 2>&1; then
          nvidia-smi --query-gpu=name,memory.total --format=csv,noheader | sed 's/^/    /' || true
        fi
      elif [ -d /dev/dri ]; then
        dri_count=$(ls /dev/dri/ | grep -c "card" || echo "0")
        echo -e "  ${GREEN}SUCCESS: DRI devices: $dri_count found${NC}"
      else
        echo -e "  INFO: No GPU devices detected"
      fi
      
      echo ""
      echo -e "${BLUE}HEALTH CHECK SUMMARY:${NC}"
      if systemctl is-active rke2-server.service >/dev/null 2>&1 || systemctl is-active rke2-agent.service >/dev/null 2>&1; then
        echo -e "${GREEN}SUCCESS: RKE2 deployment appears healthy${NC}"
        echo -e "   Monitor progress with: ${YELLOW}journalctl -u rke2-server.service -f${NC}"
      else
        echo -e "${RED}ERROR: RKE2 deployment needs attention${NC}"
        echo -e "   Check logs with: ${YELLOW}journalctl -u rke2-server.service -n 50${NC}"
      fi
      
      echo -e "${BLUE}Useful commands:${NC}"
      echo -e "   kubectl get nodes: ${YELLOW}kubectl get nodes -o wide${NC}"
      echo -e "   Check pods: ${YELLOW}kubectl get pods -A${NC}"
      echo -e "   RKE2 logs: ${YELLOW}journalctl -u rke2-server.service -f${NC}"
      echo ""
      echo -e "${GREEN}SUCCESS: LXC RKE2 health check complete${NC}"
    mode: 0755
    owner: root
    group: root

# Enhanced deployment summary
- name: Display LXC deployment summary
  ansible.builtin.debug:
    msg: |
      
      LXC-OPTIMIZED RKE2 CONFIGURATION APPLIED
      ========================================
      
      RKE2 VERSION: v1.33.3+rke2r1 (Kubernetes v1.33.3)
      
      APPLIED CONFIGURATIONS:
      - Enhanced RKE2 Config: Integrated into main playbook configuration
      - Kubelet Settings: Comprehensive config file with LXC optimizations
      - Network Stack: Bridge netfilter, IP forwarding, conntrack optimization
      - Sysctl Tuning: Container-optimized kernel parameters
      - Device Access: /dev/kmsg created for kubelet compatibility
      - NVIDIA Container Toolkit: {{ 'Installed directly for LXC compatibility' if has_nvidia_devices | default(false) else 'Skipped (no GPU detected)' }}
      
      PERFORMANCE OPTIMIZATIONS:
      - Resource Limits: System and Kube reserved resources configured
      - Connection Limits: Increased netdev backlog and connection tracking
      - Memory Management: Optimized vm.max_map_count and inotify limits
      - Process Limits: Enhanced pid_max and threads-max for dense workloads
      
      SECURITY ENHANCEMENTS:
      - Authentication: Token webhook and proper CA configuration
      - Authorization: Webhook-based authorization mode
      - Admission Control: NodeRestriction, ResourceQuota, LimitRanger
      - Anonymous Access: Disabled for security
      
      GPU PASSTHROUGH BENEFITS:
      - Low Overhead: LXC containers vs KVM VMs for GPU workloads
      - Device Plugins: Enabled for GPU device management
      - Performance: Near-native GPU performance with minimal virtualization overhead
      - NVIDIA Runtime: {{ 'Configured directly in containerd for LXC compatibility' if has_nvidia_devices | default(false) else 'Not configured (no GPU detected)' }}
      - Container Toolkit: {{ 'Installed and configured for immediate GPU workload support' if has_nvidia_devices | default(false) else 'Available for future GPU node addition' }}
      
      DEPLOYMENT NOTES:
      - Startup Time: LXC containers may take 2-3 minutes for full initialization
      - Health Check: Use /usr/local/bin/rke2-lxc-health-check.sh for validation
      - Monitoring: Check 'kubectl get nodes' once API server is ready
      - Troubleshooting: Monitor with 'journalctl -u rke2-server.service -f'
      
      Ready for RKE2 deployment!

# NVIDIA GPU Validation Check (runs after RKE2 setup) - Only on explicitly configured GPU nodes
- name: Comprehensive NVIDIA GPU and Container Runtime Validation
  ansible.builtin.shell: |
    echo "NVIDIA GPU AND CONTAINER RUNTIME VALIDATION"
    echo "==========================================="
    
    # 1. Basic NVIDIA driver validation
    echo "1. NVIDIA Driver Validation:"
    echo "----------------------------"
    if command -v nvidia-smi >/dev/null 2>&1; then
      nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader,nounits
      echo "✓ nvidia-smi working correctly"
    else
      echo "✗ nvidia-smi not available"
      exit 1
    fi
    echo ""
    
    # 2. NVIDIA Container Toolkit validation
    echo "2. NVIDIA Container Toolkit Validation:"
    echo "---------------------------------------"
    if command -v nvidia-container-runtime >/dev/null 2>&1; then
      echo "✓ nvidia-container-runtime binary available"
      nvidia-container-runtime --version || echo "Note: version check may not be supported"
    else
      echo "✗ nvidia-container-runtime not found"
      exit 1
    fi
    
    if command -v nvidia-ctk >/dev/null 2>&1; then
      echo "✓ nvidia-ctk utility available"
    else
      echo "⚠ nvidia-ctk not found (may not be critical)"
    fi
    echo ""
    
    # 3. Containerd runtime configuration validation
    echo "3. Containerd Runtime Configuration:"
    echo "-----------------------------------"
    if [ -f "/var/lib/rancher/rke2/agent/etc/containerd/config.toml" ]; then
      if grep -q "nvidia.*runtime" /var/lib/rancher/rke2/agent/etc/containerd/config.toml; then
        echo "✓ NVIDIA runtime configured in containerd"
        echo "NVIDIA runtime configuration:"
        grep -A5 -B1 "nvidia.*runtime" /var/lib/rancher/rke2/agent/etc/containerd/config.toml || echo "Configuration found but format may vary"
      else
        echo "⚠ NVIDIA runtime not found in containerd config"
        echo "Checking if containerd is using template..."
        if [ -f "/var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl" ]; then
          if grep -q "nvidia.*runtime" /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl; then
            echo "✓ NVIDIA runtime found in containerd template (will be applied on restart)"
          else
            echo "✗ NVIDIA runtime not configured in containerd template"
          fi
        fi
      fi
    else
      echo "✗ Containerd config file not found"
    fi
    echo ""
    
    # 4. GPU device accessibility validation
    echo "4. GPU Device Accessibility:"
    echo "---------------------------"
    gpu_count=0
    for device in /dev/nvidia*; do
      if [ -c "$device" ]; then
        echo "✓ $device is accessible"
        gpu_count=$((gpu_count + 1))
      fi
    done
    
    if [ "$gpu_count" -gt 0 ]; then
      echo "✓ Found $gpu_count NVIDIA device(s)"
    else
      echo "✗ No NVIDIA character devices found"
    fi
    echo ""
    
    # 5. Container runtime test (if RKE2 is running)
    echo "5. Container Runtime Test:"
    echo "-------------------------"
    if systemctl is-active --quiet rke2-server || systemctl is-active --quiet rke2-agent; then
      echo "✓ RKE2 is running - container runtime should be active"
      
      # Test if we can list runtimes
      if command -v ctr >/dev/null 2>&1; then
        echo "Available container runtimes:"
        ctr --address /run/k3s/containerd/containerd.sock --namespace k8s.io version 2>/dev/null || \
        ctr --address /var/run/containerd/containerd.sock --namespace k8s.io version 2>/dev/null || \
        echo "Note: Could not connect to containerd socket (may be normal during startup)"
      else
        echo "Note: ctr command not available for runtime testing"
      fi
    else
      echo "ℹ RKE2 not yet running - container runtime test will be available after RKE2 starts"
    fi
    echo ""
    
    echo "========================================"
    echo "NVIDIA VALIDATION COMPLETE"
    echo "========================================"
    echo "✓ GPU hardware accessible"
    echo "✓ NVIDIA drivers functional" 
    echo "✓ Container toolkit installed"
    echo "✓ Containerd runtime configured"
    echo ""
    echo "Ready for GPU workloads!"
    
  register: nvidia_validation_result
  changed_when: false
  when: >
    (inventory_hostname in groups.get('gpu_nodes', [])) or 
    (is_gpu_node | default(false)) or
    (ansible_hostname | regex_search(gpu_node_pattern | default('gpu')))

- name: Display NVIDIA validation results
  ansible.builtin.debug:
    var: nvidia_validation_result.stdout_lines
  when: >
    (inventory_hostname in groups.get('gpu_nodes', [])) or 
    (is_gpu_node | default(false)) or
    (ansible_hostname | regex_search(gpu_node_pattern | default('gpu')))
