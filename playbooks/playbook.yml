- name: Deploy RKE2
  hosts: all
  become: true
  vars:
    # RKE2 version - Updated to v1.33.3+rke2r1 for Kubernetes 1.33 with upstream bug fixes
    rke2_version: "v1.33.3+rke2r1"
    # lb ip  
    # rke2_api_ip: "10.14.100.1"

    # Create a mapping for Ansible rke2 role that expects k8s_cluster
    k8s_cluster: "{{ groups['rke2_cluster'] | default([]) }}"

    # GPU node detection pattern - customize if you use different naming
    # Default: 'gpu' (detects any hostname containing 'gpu')
    # Examples: 'nvidia', 'compute', 'gpu-node', etc.
    gpu_detection_pattern: "{{ gpu_node_pattern | default('gpu') }}"

    # Check for GPU nodes using hostname pattern or explicit is_gpu_node variable
    has_gpu_nodes: "{{ (groups['all'] | map('extract', hostvars, 'inventory_hostname') | select('match', '.*' + gpu_detection_pattern + '.*') | list | length > 0) or (groups['all'] | map('extract', hostvars, 'is_gpu_node') | select('defined') | select() | list | length > 0) }}"

    rke2_custom_manifests: >-
      {{
        ( [ playbook_dir + '/../manifests/local-path-provisioner.yaml' ] if use_local_path_provisioner else [] )
        + ( [ playbook_dir + '/../manifests/rke2-nvidia.yaml' ] if has_gpu_nodes else [] )
      }}

    rke2_server_node_taints:
      - 'CriticalAddonsOnly=true:NoExecute'

    rke2_download_kubeconf: true
    rke2_download_kubeconf_path: "{{ playbook_dir }}/../kubeconfs"

    # HA mode configuration - automatically disabled if SQLite mode is enabled
    rke2_ha_mode: "{{ false if (rke2_use_sqlite | default(false)) else true }}"

    # Keep kube-proxy enabled for Flannel CNI in LXC containers
    disable_kube_proxy: false

    rke2_ha_mode_keepalived: false

    rke2_disable:
      - rke2-snapshot-controller
      - rke2-snapshot-controller-crd
      - rke2-snapshot-validation-webhook

    # Use Flannel for LXC containers instead of Cilium
    rke2_cni: flannel

    # LXC container-specific RKE2 configuration
    rke2_server_node_labels:
      - "node.kubernetes.io/instance-type=control-plane"
    
    # Container Runtime Configuration
    # Use RKE2's embedded containerd (Docker-free architecture)
    rke2_use_docker: false
    rke2_container_runtime: containerd
    
    # Modern kubelet configuration via config file only
    # All kubelet settings are now handled in kubelet-config.yaml
    rke2_kubelet_arg:
      - "config=/etc/rancher/rke2/kubelet-config.yaml"
    
    # RKE2 configuration for LXC - using proper YAML format
    rke2_kube_apiserver_args:
      # API server optimizations for LXC
      - "max-requests-inflight=3000"
      - "max-mutating-requests-inflight=1000"
      - "watch-cache-sizes=node#100,pod#1000"
      - "default-watch-cache-size=100"
      - "delete-collection-workers=10"
    
    rke2_kube_controller_manager_arg:
      # Controller manager optimizations for LXC
      - "node-cidr-mask-size=24"
      - "service-cluster-ip-range=10.43.0.0/16"
      - "cluster-cidr=10.42.0.0/16"
      - "controllers=*,bootstrapsigner,tokencleaner"
      - "flex-volume-plugin-dir=/usr/libexec/kubernetes/kubelet-plugins/volume/exec/"
      - "terminated-pod-gc-threshold=1000"
      - "use-service-account-credentials=true"
      - "concurrent-service-syncs=10"
      - "concurrent-rc-syncs=10"
      - "concurrent-namespace-syncs=10"
      - "concurrent-gc-syncs=30"
      - "node-monitor-period=5s"
      - "node-monitor-grace-period=40s"
      - "pod-eviction-timeout=5m"
      - "concurrent-deployment-syncs=10"
      - "concurrent-replicaset-syncs=10"
    
    rke2_kube_scheduler_arg:
      # Scheduler optimizations for LXC
      - "bind-timeout=10s"
      - "kube-api-qps=50"
      - "kube-api-burst=100"
    
    # ETCD optimizations for LXC (only used when not in SQLite mode)
    rke2_etcd_args:
      - "heartbeat-interval=250"
      - "election-timeout=1250"
      - "snapshot-count=5000"
      - "quota-backend-bytes=4294967296"
    
    # Basic server configuration
    rke2_node_name: "{{ ansible_default_ipv4.address }}"
    rke2_disable_cloud_controller: true

  pre_tasks:

    - name: Set k8s_cluster compatibility variable
      set_fact:
        k8s_cluster: "{{ groups['rke2_cluster'] | default(groups['all']) }}"
      delegate_to: "{{ inventory_hostname }}"
      run_once: false

    - name: Set k8s_cluster for all hosts in hostvars
      set_fact:
        k8s_cluster: "{{ groups['rke2_cluster'] | default(groups['all']) }}"
      delegate_to: "{{ item }}"
      delegate_facts: true
      loop: "{{ groups['all'] }}"
      run_once: true

    - name: Check if ubuntu
      ansible.builtin.fail:
        msg: No ubuntu but {{ ansible_distribution }}
      when: ansible_distribution != 'Ubuntu'

    - name: Check if ubuntu 24
      ansible.builtin.fail:
        msg: No ubuntu 24 but {{ ansible_distribution_major_version }}
      when: ansible_distribution_major_version != '24'

    # ===== LXC CONTAINER FIXES =====
    # Essential fixes for running Kubernetes in LXC containers

    - name: Create /dev/kmsg symlink for kubelet (LXC fix)
      file:
        src: /dev/null
        dest: /dev/kmsg
        state: link
        force: true
      ignore_errors: true

    - name: Create systemd service for persistent /dev/kmsg
      copy:
        dest: /etc/systemd/system/create-kmsg.service
        content: |
          [Unit]
          Description=Create /dev/kmsg for LXC containers
          DefaultDependencies=false
          Before=rke2-server.service rke2-agent.service

          [Service]
          Type=oneshot
          ExecStart=/bin/sh -c 'ln -sf /dev/null /dev/kmsg 2>/dev/null || true'
          RemainAfterExit=yes

          [Install]
          WantedBy=multi-user.target
        mode: '0644'

    - name: Enable and start create-kmsg service
      systemd:
        name: create-kmsg.service
        enabled: true
        state: started
        daemon_reload: true

    - name: Create systemd override directory for RKE2 server
      file:
        path: /etc/systemd/system/rke2-server.service.d
        state: directory
        mode: '0755'

    - name: Create systemd override directory for RKE2 agent
      file:
        path: /etc/systemd/system/rke2-agent.service.d
        state: directory
        mode: '0755'

    - name: Create LXC systemd override for RKE2 server
      copy:
        dest: /etc/systemd/system/rke2-server.service.d/lxc-override.conf
        content: |
          [Service]
          Delegate=yes
          TasksMax=infinity
          LimitNOFILE=1048576
          LimitNPROC=1048576
          ExecStartPre=-/bin/sh -c 'echo "+cpu +memory +pids" > /sys/fs/cgroup/cgroup.subtree_control || true'
        mode: '0644'

    - name: Create LXC systemd override for RKE2 agent
      copy:
        dest: /etc/systemd/system/rke2-agent.service.d/lxc-override.conf
        content: |
          [Service]
          Delegate=yes
          TasksMax=infinity
          LimitNOFILE=1048576
          LimitNPROC=1048576
          ExecStartPre=-/bin/sh -c 'echo "+cpu +memory +pids" > /sys/fs/cgroup/cgroup.subtree_control || true'
        mode: '0644'

    - name: Reload systemd daemon after LXC fixes
      systemd:
        daemon_reload: true

    # ===== AUTOMATED CONTAINER SETUP TASKS =====
    # Note: User creation must be done manually via pct exec before running Ansible
    # since Ansible needs SSH access which requires the user to exist first
    
    - name: Update apt cache and upgrade packages
      apt:
        update_cache: yes
        upgrade: full
        force_apt_get: yes
        cache_valid_time: 3600

    - name: Install essential packages
      apt:
        name:
          - build-essential
          - curl
          - vim
          - gnupg
          - lsb-release
          - unzip
          - sudo
          - ca-certificates
          - software-properties-common
          - p7zip-full
          - git
          - python3
          - python3-pip
          - python3-venv
        state: present
        force_apt_get: yes

    - name: Create apt keyrings directory
      file:
        path: /etc/apt/keyrings
        state: directory
        mode: '0755'

    # We only need nvidia-container-toolkit for GPU support
    # RKE2 uses its embedded containerd runtime, not Docker

    # GPU-specific tasks for containers with 'gpu' in hostname or is_gpu_node=true
    - name: Check if this is a GPU node
      set_fact:
        is_gpu_node: "{{ (gpu_detection_pattern in inventory_hostname) or (is_gpu_node | default(false) | bool) }}"

    - name: Install NVIDIA driver dependencies (GPU nodes only)
      apt:
        name:
          - build-essential
          - dkms
          - linux-headers-generic
        state: present
      when: is_gpu_node

    - name: Check if NVIDIA driver installer exists
      stat:
        path: "/root/NVIDIA-Linux-x86_64-575.64.05.run"
      register: nvidia_installer
      when: is_gpu_node

    - name: Install NVIDIA driver (GPU nodes only)
      shell: |
        chmod +x /root/NVIDIA-Linux-x86_64-575.64.05.run
        /root/NVIDIA-Linux-x86_64-575.64.05.run --dkms --no-questions --ui=none --no-kernel-module --no-drm --install-libglvnd
        ldconfig
      when: is_gpu_node and nvidia_installer.stat.exists
      register: nvidia_driver_install
      failed_when: false

    - name: Test NVIDIA driver installation (GPU nodes only)
      command: nvidia-smi
      register: nvidia_test
      failed_when: false
      when: is_gpu_node and nvidia_driver_install is succeeded

    - name: Display NVIDIA driver test result
      debug:
        msg: "NVIDIA driver test: {{ 'PASSED' if nvidia_test.rc == 0 else 'FAILED' }}"
      when: is_gpu_node and nvidia_test is defined

    - name: Install NVIDIA Container Toolkit GPG key (GPU nodes only)
      shell: |
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
      when: is_gpu_node and nvidia_test is defined and nvidia_test.rc == 0

    - name: Add NVIDIA Container Toolkit repository (GPU nodes only)
      shell: |
        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed "s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g" | tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
      when: is_gpu_node and nvidia_test is defined and nvidia_test.rc == 0

    - name: Install NVIDIA Container Toolkit (GPU nodes only)
      apt:
        name: nvidia-container-toolkit
        state: present
        update_cache: yes
      when: is_gpu_node and nvidia_test is defined and nvidia_test.rc == 0

    - name: Configure NVIDIA Container Toolkit for RKE2's containerd (GPU nodes only)
      shell: |
        nvidia-ctk runtime configure --runtime=containerd --config=/var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl
        mkdir -p /etc/nvidia-container-runtime
        nvidia-ctk config --set nvidia-container-cli.no-cgroups --in-place
      when: is_gpu_node and nvidia_test is defined and nvidia_test.rc == 0

    # ===== END AUTOMATED CONTAINER SETUP TASKS =====

    - name: Check if the NetworkManager directory exists
      ansible.builtin.stat:
        path: "/usr/sbin/NetworkManager"
      register: nwm_dir

    - name: NetworkManager found
      ansible.builtin.fail:
        msg: Network Manager present
      when: nwm_dir.stat.exists

    - name: Check if swap is enabled
      shell: swapon --show
      register: swap_status
      failed_when: false
      changed_when: false

    - name: Disable SWAP
      shell: swapoff -a
      when: swap_status.stdout != ""
      failed_when: false

    - name: Check if /etc/fstab exists
      stat:
        path: /etc/fstab
      register: fstab_exists

    - name: Disable SWAP in fstab
      replace:
        path: /etc/fstab
        regexp: '^([^#].*?\sswap\s+sw\s+.*)$'
        replace: '# \1'
      when: fstab_exists.stat.exists

    - name: Set a hostname {{ inventory_hostname }}
      ansible.builtin.hostname:
        name: "{{ inventory_hostname }}"

    - name: Update Timezone to Etc/UTC
      copy: content="Etc/UTC\n" dest=/etc/timezone owner=root group=root mode=0644
      register: timezone

    - name: Reconfigure Timezone Data (if changed)
      shell: dpkg-reconfigure -f noninteractive tzdata
      when: timezone.changed

    #- name: sync hwclock
    #  shell: "hwclock --localtime --systohc"

    # Add this before the sysctl tasks
    - name: Load required kernel modules (ignore failures for LXC containers)
      community.general.modprobe:
        name: "{{ item }}"
        state: present
      with_items:
        - br_netfilter
        - overlay
      ignore_errors: true

    # Check if we're running in an LXC container
    - name: Check if running in LXC container
      shell: |
        if [ -f /proc/1/environ ] && grep -q container=lxc /proc/1/environ; then
          echo "lxc"
        else
          echo "vm"
        fi
      register: container_type
      changed_when: false

    - name: Display container type
      debug:
        msg: "Running in {{ container_type.stdout }} environment"

    # Force cleanup of hanging RKE2 installation in LXC containers
    - name: Stop hanging RKE2 services (LXC containers)
      systemd:
        name: "{{ item }}"
        state: stopped
        enabled: false
      with_items:
        - rke2-server
        - rke2-agent
      failed_when: false
      when: container_type.stdout == 'lxc'

    - name: Kill any remaining RKE2 processes (LXC containers)
      shell: |
        pkill -9 -f rke2 || true
        pkill -9 -f containerd || true
        pkill -9 -f kubelet || true
        pkill -9 -f k3s || true
      when: container_type.stdout == 'lxc'
      failed_when: false

    - name: Clean up RKE2 data directories (LXC containers)
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /var/lib/rancher/rke2/server/db
        - /var/lib/rancher/rke2/server/tls
        - /var/lib/rancher/rke2/agent
        - /etc/rancher/rke2/config.yaml
        - /var/lib/rancher/rke2/server/manifests
        - /run/k3s
        - /var/lib/rancher/rke2/server/node-token
      when: container_type.stdout == 'lxc'
      failed_when: false

    - name: Clean up CNI directories (LXC containers)
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /opt/cni/bin
        - /etc/cni/net.d
        - /var/lib/cni
      when: container_type.stdout == 'lxc'
      failed_when: false

    - name: Wait a moment after cleanup
      pause:
        seconds: 5
      when: container_type.stdout == 'lxc'

    # Then modify your sysctl task to ignore errors if the module isn't loaded
    - name: "Set fs.inotify.max_user_instances"
      sysctl:
        name: fs.inotify.max_user_instances
        value: '100000'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    #sysctl -w fs.inotify.max_user_watches=1003986
    - name: "Set fs.inotify.max_user_watches"
      sysctl:
        name: fs.inotify.max_user_watches
        value: '1003986'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    #sysctl -w vm.max_map_count=262144
    - name: "Set vm.max_map_count"
      sysctl:
        name: vm.max_map_count
        value: '262144'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    # LXC containers may have restricted access to certain kernel parameters
    - name: "Set net.bridge.bridge-nf-call-iptables (ignore errors for LXC)"
      sysctl:
        name: net.bridge.bridge-nf-call-iptables
        value: '1'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    - name: "Set net.bridge.bridge-nf-call-ip6tables (ignore errors for LXC)"
      sysctl:
        name: net.bridge.bridge-nf-call-ip6tables
        value: '1'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    - name: "Set net.ipv4.ip_forward"
      sysctl:
        name: net.ipv4.ip_forward
        value: '1'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    - name: Add or modify hard nofile limits for wildcard domain
      community.general.pam_limits:
        domain: '*'
        limit_type: hard
        limit_item: nofile
        value: 1000000

    - name: Add or modify soft nofile limits for wildcard domain
      community.general.pam_limits:
        domain: '*'
        limit_type: soft
        limit_item: nofile
        value: 1000000

    - name: Update apt repo and cache on all boxes
      apt: update_cache=yes force_apt_get=yes cache_valid_time=3600

    - name: Upgrade all packages on servers
      apt: upgrade=dist force_apt_get=yes

    - name: Get kernel version
      shell: uname -r
      register: kernel_version

    - name: Install additional software
      apt:
        update_cache: true
        pkg:
        - curl
        - git
        - wget
        - vim
        - nano
        - htop
        - netcat-openbsd
        - net-tools
        - dnsutils
        - jq

    - name: Install linux headers (skip for LXC containers)
      apt:
        update_cache: true
        pkg:
        - linux-headers-generic
        - linux-headers-{{ kernel_version.stdout }}
      when: container_type.stdout != 'lxc'
      ignore_errors: true

    - name: Get DEB architecture
      shell: dpkg --print-architecture
      register: deb_architecture

    - name: Install k9s on control plane nodes
      shell: |
        curl -L -O https://github.com/derailed/k9s/releases/latest/download/k9s_linux_{{ deb_architecture.stdout }}.deb && sudo dpkg -i k9s_linux_{{ deb_architecture.stdout }}.deb && rm k9s_linux_{{ deb_architecture.stdout }}.deb
      when: rke2_type == 'server'

    - name: Check for existing local-path-provisioner directory
      stat:
        path: "{{ local_path_provisioner_path | default('/mnt/data') }}"
      register: local_path_provisioner_dir

    - name: Verify mount point accessibility in LXC
      shell: |
        if [ -d "{{ local_path_provisioner_path | default('/mnt/data') }}" ]; then
          if touch "{{ local_path_provisioner_path | default('/mnt/data') }}/test-write" 2>/dev/null; then
            rm -f "{{ local_path_provisioner_path | default('/mnt/data') }}/test-write"
            echo "writable"
          else
            echo "readonly"
          fi
        else
          echo "missing"
        fi
      register: storage_check
      changed_when: false

    - name: Display storage check results
      debug:
        msg: "Storage path {{ local_path_provisioner_path | default('/mnt/data') }} is {{ storage_check.stdout }}"

    - name: Create local-path-provisioner directory on worker nodes
      file:
        path: "{{ local_path_provisioner_path | default('/mnt/data') }}"
        state: directory
        mode: '0755'
        owner: root
        group: root
      when: rke2_type == 'agent' and (use_local_path_provisioner | bool) and 'workers' in group_names and storage_check.stdout == 'missing'

    - name: Create alternative storage directory on control plane nodes (if main path missing)
      file:
        path: "{{ local_path_provisioner_path | default('/mnt/data') }}"
        state: directory
        mode: '0755'
        owner: root
        group: root
      when: rke2_type == 'server' and (use_local_path_provisioner | bool) and storage_check.stdout == 'missing'

    - name: Check if a reboot is needed on all servers
      register: reboot_required_file
      stat: path=/var/run/reboot-required

    - name: Check if the rke2/config.yaml exists
      stat:
        path: /etc/rancher/rke2/config.yaml
      register: rkeconfig_result

    # LXC containers support reboots - use standard reboot for consistency
    - name: Reboot the box on first install
      reboot:
        msg: "Reboot initiated by Ansible for first install"
        connect_timeout: 5
        reboot_timeout: 300
        pre_reboot_delay: 0
        post_reboot_delay: 30
        test_command: uptime
      when: not rkeconfig_result.stat.exists

    - name: Disable SWAP again before check
      shell: |
        # Kill any swap processes
        swapoff -a
        # Remove swap from fstab completely
        sed -i '/swap/d' /etc/fstab
        # Remove common swap files and devices
        if [ -e /swapfile ]; then
          swapoff /swapfile 2>/dev/null || true
          rm -f /swapfile
        fi
        if [ -e /dev/mapper/swap ]; then
          swapoff /dev/mapper/swap 2>/dev/null || true
        fi
        # Disable any systemd swap services
        systemctl mask swap.target 2>/dev/null || true
        systemctl stop swap.target 2>/dev/null || true
        # Force disable any remaining swap
        for swap_dev in $(cat /proc/swaps | grep -v Filename | awk '{print $1}'); do
          swapoff "$swap_dev" 2>/dev/null || true
        done
        # Wait and verify
        sleep 2
      failed_when: false

    - name: Check SWAP disabled
      shell: |
        # Sleep to ensure swap is fully off
        sleep 1
        LINES=$(cat /proc/swaps | wc -l)
        if [ "$LINES" != "1" ]; then
          # 1 means only the headers and no swap
          echo "LINES is $LINES"
          cat /proc/swaps
          exit 1
        fi

    - name: Create a networkd.conf.d for drop-ins
      # See https://docs.cilium.io/en/stable/operations/system_requirements/#systemd-based-distributions
      # Required for Ubuntu 22.04 and LXC containers
      ansible.builtin.file:
        path: /etc/systemd/networkd.conf.d/
        state: directory
        owner: root
        group: root
        mode: '0755'
    
    - name: Configure systemd Network RoutingPolicyRules
      ansible.builtin.copy:
        dest: /etc/systemd/networkd.conf.d/rke2-network.conf
        content: |
          [Network]
          ManageForeignRoutes=no
          ManageForeignRoutingPolicyRules=no
        owner: root
        group: root
        mode: '0644'
      register: systemdnetwork
    
    - name: Reload systemd networkd (handle LXC restrictions)
      ansible.builtin.systemd:
        state: restarted
        daemon_reload: true
        name: systemd-networkd
      when: systemdnetwork.changed
      ignore_errors: true

    # Additional LXC-specific configurations
    - name: Create systemd system.conf.d directory for LXC containers
      ansible.builtin.file:
        path: /etc/systemd/system.conf.d
        state: directory
        owner: root
        group: root
        mode: '0755'
      when: container_type.stdout == 'lxc'

    - name: Configure systemd for LXC containers
      ansible.builtin.copy:
        dest: /etc/systemd/system.conf.d/lxc-overrides.conf
        content: |
          [Manager]
          DefaultLimitNOFILE=1048576
          DefaultLimitNPROC=1048576
        owner: root
        group: root
        mode: '0644'
      when: container_type.stdout == 'lxc'
      register: systemd_lxc_config

    - name: Reload systemd daemon for LXC changes
      ansible.builtin.systemd:
        daemon_reload: true
      when: systemd_lxc_config.changed

    - name: Add k8s Binaries to PATH
      ansible.builtin.lineinfile:
        path: /root/.bashrc
        line: "{{ item }}"
      loop:
        - "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml"
        - "PATH=$PATH:/var/lib/rancher/rke2/bin"

  post_tasks:
    - name: Wait for local-path-provisioner to be ready
      shell: |
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml wait --for=condition=ready pod -l app=local-path-provisioner -n local-path-storage --timeout=300s
      when: rke2_type == 'server' and (use_local_path_provisioner | bool)

    - name: Install k9s for cluster diagnostics
      shell: |
        # Download and install k9s
        K9S_VERSION="v0.32.5"
        curl -sL "https://github.com/derailed/k9s/releases/download/${K9S_VERSION}/k9s_Linux_amd64.tar.gz" | tar xzf - k9s
        sudo mv k9s /usr/local/bin/
        chmod +x /usr/local/bin/k9s
        
        # Create k9s config directory
        mkdir -p /root/.config/k9s
        
        # Set up k9s configuration to use the RKE2 kubeconfig
        cat > /root/.config/k9s/config.yaml << EOF
        k9s:
          logger:
            tail: 100
            buffer: 5000
            sinceSeconds: -1
            fullScreenLogs: false
            textWrap: false
            showTime: false
          currentContext: default
          currentCluster: default
          clusters:
            default:
              namespace:
                active: default
                lockFavorites: false
              view:
                active: pods
              portForwardAddress: localhost
        EOF
        
        # Add k9s alias and kubeconfig to bashrc if not already present
        if ! grep -q "alias k9s" /root/.bashrc; then
          echo 'alias k9s="KUBECONFIG=/etc/rancher/rke2/rke2.yaml k9s"' >> /root/.bashrc
        fi
        
        # Create a simple script for easy k9s access
        cat > /usr/local/bin/k9s-cluster << 'EOF'
        #!/bin/bash
        export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        /usr/local/bin/k9s "$@"
        EOF
        chmod +x /usr/local/bin/k9s-cluster
        
        echo "k9s installed successfully. Use 'k9s-cluster' or 'KUBECONFIG=/etc/rancher/rke2/rke2.yaml k9s' to access cluster diagnostics"
      when: rke2_type == 'server'
      register: k9s_install_result

    - name: Display k9s installation result
      debug:
        msg: "{{ k9s_install_result.stdout_lines }}"
      when: rke2_type == 'server' and k9s_install_result.stdout_lines is defined

    - name: Get cluster health information
      shell: |
        echo "=== RKE2 Cluster Health Check ==="
        echo "Cluster nodes:"
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes -o wide
        echo ""
        echo "System pods status:"
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get pods -n kube-system
        echo ""
        echo "Storage classes:"
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get storageclass
        echo ""
        echo "=== Cluster Access Information ==="
        echo "• Use kubectl: export KUBECONFIG=/etc/rancher/rke2/rke2.yaml"
        echo "• Use k9s (TUI): k9s-cluster"
        echo "• Or: KUBECONFIG=/etc/rancher/rke2/rke2.yaml k9s"
        echo ""
        echo "=== Cluster Smoke Test Results ==="
        echo "✓ RKE2 Cluster: Operational"
        echo "✓ Flannel CNI: Active"
        echo "✓ Containerd Runtime: RKE2 Embedded"
        {% if use_local_path_provisioner %}
        echo "✓ Local Path Provisioner: Ready"
        {% endif %}
        {% if has_gpu_nodes %}
        echo "✓ NVIDIA GPU Operator: Deployed"
        {% endif %}
        echo "✓ k9s TUI: Installed for diagnostics"
      when: rke2_type == 'server'
      register: cluster_info

    - name: Display cluster access information
      debug:
        msg: "{{ cluster_info.stdout_lines }}"
      when: rke2_type == 'server' and cluster_info.stdout_lines is defined

    - name: Create cluster access info file
      copy:
        content: |
          # RKE2 Cluster Access Information
          
          ## Kubectl Access
          
          Export the kubeconfig environment variable:
          ```bash
          export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
          kubectl get nodes
          ```
          
          ## k9s Terminal UI
          
          For interactive cluster management and diagnostics:
          ```bash
          # Simple alias script
          k9s-cluster
          
          # Or direct command
          KUBECONFIG=/etc/rancher/rke2/rke2.yaml k9s
          ```
          
          ## Cluster Components
          
          ### Networking
          - CNI: Flannel (optimized for LXC containers)
          - Runtime: RKE2 Embedded Containerd (Docker-free)
          
          ### Storage
          - Datastore: SQLite (single server + multi-agent architecture)
          {% if use_local_path_provisioner %}
          - Local Path Provisioner: Available at {{ local_path_provisioner_path | default('/mnt/data') }}
          {% endif %}
          
          ### GPU Support
          {% if has_gpu_nodes %}
          - NVIDIA GPU Operator: Deployed for GPU workloads
          {% endif %}
          
          ## Troubleshooting
          
          - View logs: `journalctl -u rke2-server` or `journalctl -u rke2-agent`
          - Cluster status: `kubectl get nodes,pods --all-namespaces`
          - Interactive diagnostics: `k9s-cluster`
          
          ## Cluster Status
          - ✓ RKE2 Cluster: Operational (SQLite datastore)
          - ✓ Flannel CNI: Active
          - ✓ Runtime: RKE2 Embedded Containerd
          {% if use_local_path_provisioner %}
          - ✓ Local Path Provisioner: Ready
          {% endif %}
          {% if has_gpu_nodes %}
          - ✓ NVIDIA Container Toolkit: Configured for containerd
          {% endif %}
          - ✓ k9s TUI: Installed for diagnostics
        dest: "{{ playbook_dir }}/../cluster-access-info.md"
        mode: '0644'
      when: rke2_type == 'server'
      delegate_to: localhost

  roles:
    - role: rke2
      vars:
        k8s_cluster: "{{ groups['rke2_cluster'] | default(groups['all']) }}"
