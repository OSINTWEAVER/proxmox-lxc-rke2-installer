- name: Deploy RKE2
  hosts: all
  become: true
  vars:
    # RKE2 version - Updated to v1.33.3+rke2r1 for Kubernetes 1.33 with upstream bug fixes
    rke2_version: "v1.33.3+rke2r1"
    # lb ip  
    # rke2_api_ip: "10.14.100.1"

    # Create a mapping for Ansible rke2 role that expects k8s_cluster
    k8s_cluster: "{{ groups['rke2_cluster'] | default([]) }}"

    # GPU node detection pattern - customize if you use different naming
    # Default: 'gpu' (detects any hostname containing 'gpu')
    # Examples: 'nvidia', 'compute', 'gpu-node', etc.
    gpu_detection_pattern: "{{ gpu_node_pattern | default('gpu') }}"

    # Check for GPU nodes using hostname pattern or explicit is_gpu_node variable
    has_gpu_nodes: "{{ (groups['all'] | map('extract', hostvars, 'inventory_hostname') | select('match', '.*' + gpu_detection_pattern + '.*') | list | length > 0) or (groups['all'] | map('extract', hostvars, 'is_gpu_node') | select('defined') | select() | list | length > 0) }}"

    rke2_custom_manifests: >-
      {{
        ( [ playbook_dir + '/../manifests/local-path-provisioner.yaml' ] if use_local_path_provisioner else [] )
        + ( [ playbook_dir + '/../manifests/rke2-nvidia.yaml' ] if has_gpu_nodes else [] )
      }}

    # Removed CriticalAddonsOnly taint as it prevents CNI daemonsets from scheduling
    # rke2_server_node_taints:
    #   - 'CriticalAddonsOnly=true:NoExecute'

    rke2_download_kubeconf: true
    rke2_download_kubeconf_path: "{{ playbook_dir }}/../kubeconfs"

    # HA mode configuration - automatically disabled if SQLite mode is enabled
    rke2_ha_mode: "{{ false if (rke2_use_sqlite | default(false)) else true }}"

    # Keep kube-proxy enabled for Flannel CNI in LXC containers
    disable_kube_proxy: false

    rke2_ha_mode_keepalived: false

    rke2_disable:
      - rke2-snapshot-controller
      - rke2-snapshot-controller-crd
      - rke2-snapshot-validation-webhook

    # Use Flannel for LXC containers instead of Cilium
    rke2_cni: flannel

    # LXC container-specific RKE2 configuration
    rke2_server_node_labels:
      - "node.kubernetes.io/instance-type=control-plane"
    
    # Container Runtime Configuration
    # Use RKE2's embedded containerd (Docker-free architecture)
    rke2_use_docker: false
    rke2_container_runtime: containerd
    
    # Modern kubelet configuration via config file only
    # All kubelet settings are now handled in kubelet-config.yaml
    rke2_kubelet_arg:
      - "config=/etc/rancher/rke2/kubelet-config.yaml"
      - "cloud-provider="
    
    # RKE2 configuration for LXC - let RKE2 use its built-in defaults
    # Note: Empty arrays (rke2_kube_apiserver_args: []) cause malformed --=true flags
    # By not defining these variables, RKE2 uses its proven defaults
    # Uncomment and add specific arguments only if needed:
    # rke2_kube_apiserver_args:
    #   - "some-specific-flag=value"
    # rke2_kube_controller_manager_arg:
    #   - "some-specific-flag=value" 
    # rke2_kube_scheduler_arg:
    #   - "some-specific-flag=value"
    # rke2_etcd_args:
    #   - "some-specific-flag=value"
    
    # Basic server configuration
    rke2_node_name: "{{ ansible_default_ipv4.address }}"
    rke2_disable_cloud_controller: true
    
    # Explicitly disable cloud provider to prevent uninitialized taints
    rke2_kube_apiserver_arg:
      - "cloud-provider="
    rke2_kube_controller_manager_arg:
      - "cloud-provider="

  pre_tasks:

    - name: Set k8s_cluster compatibility variable
      set_fact:
        k8s_cluster: "{{ groups['rke2_cluster'] | default(groups['all']) }}"
      delegate_to: "{{ inventory_hostname }}"
      run_once: false

    - name: Set k8s_cluster for all hosts in hostvars
      set_fact:
        k8s_cluster: "{{ groups['rke2_cluster'] | default(groups['all']) }}"
      delegate_to: "{{ item }}"
      delegate_facts: true
      loop: "{{ groups['all'] }}"
      run_once: true

    - name: Check if ubuntu
      ansible.builtin.fail:
        msg: No ubuntu but {{ ansible_distribution }}
      when: ansible_distribution != 'Ubuntu'

    - name: Check if ubuntu 24
      ansible.builtin.fail:
        msg: No ubuntu 24 but {{ ansible_distribution_major_version }}
      when: ansible_distribution_major_version != '24'

    # ===== LXC CONTAINER FIXES =====
    # Essential fixes for running Kubernetes in LXC containers

    - name: Create /dev/kmsg symlink for kubelet (LXC fix)
      file:
        src: /dev/null
        dest: /dev/kmsg
        state: link
        force: true
      ignore_errors: true

    - name: Create systemd service for persistent /dev/kmsg
      copy:
        dest: /etc/systemd/system/create-kmsg.service
        content: |
          [Unit]
          Description=Create /dev/kmsg for LXC containers
          DefaultDependencies=false
          Before=rke2-server.service rke2-agent.service

          [Service]
          Type=oneshot
          ExecStart=/bin/sh -c 'ln -sf /dev/null /dev/kmsg 2>/dev/null || true'
          RemainAfterExit=yes

          [Install]
          WantedBy=multi-user.target
        mode: '0644'

    - name: Enable and start create-kmsg service
      systemd:
        name: create-kmsg.service
        enabled: true
        state: started
        daemon_reload: true

    - name: Create systemd override directory for RKE2 server
      file:
        path: /etc/systemd/system/rke2-server.service.d
        state: directory
        mode: '0755'

    - name: Create systemd override directory for RKE2 agent
      file:
        path: /etc/systemd/system/rke2-agent.service.d
        state: directory
        mode: '0755'

    - name: Create LXC systemd override for RKE2 server
      copy:
        dest: /etc/systemd/system/rke2-server.service.d/lxc-override.conf
        content: |
          [Service]
          Delegate=yes
          TasksMax=infinity
          LimitNOFILE=1048576
          LimitNPROC=1048576
          ExecStartPre=-/bin/sh -c 'echo "+cpu +memory +pids" > /sys/fs/cgroup/cgroup.subtree_control || true'
        mode: '0644'

    - name: Create LXC systemd override for RKE2 agent
      copy:
        dest: /etc/systemd/system/rke2-agent.service.d/lxc-override.conf
        content: |
          [Service]
          Delegate=yes
          TasksMax=infinity
          LimitNOFILE=1048576
          LimitNPROC=1048576
          ExecStartPre=-/bin/sh -c 'echo "+cpu +memory +pids" > /sys/fs/cgroup/cgroup.subtree_control || true'
        mode: '0644'

    - name: Reload systemd daemon after LXC fixes
      systemd:
        daemon_reload: true

    # ===== AUTOMATED CONTAINER SETUP TASKS =====
    # Note: User creation must be done manually via pct exec before running Ansible
    # since Ansible needs SSH access which requires the user to exist first
    
    - name: Update apt cache
      apt:
        update_cache: yes
        force_apt_get: yes
        cache_valid_time: 3600

    - name: Install essential packages
      apt:
        name:
          - build-essential
          - curl
          - vim
          - gnupg
          - lsb-release
          - unzip
          - sudo
          - ca-certificates
          - software-properties-common
          - p7zip-full
          - git
        state: present
        force_apt_get: yes

    - name: Create apt keyrings directory
      file:
        path: /etc/apt/keyrings
        state: directory
        mode: '0755'

    # ===== KUBERNETES TOOLS INSTALLATION =====
    # Install kubectl, helm, and k9s for cluster management

    - name: Download and install kubectl (same version as RKE2)
      get_url:
        url: "https://dl.k8s.io/release/v1.33.3/bin/linux/amd64/kubectl"
        dest: /usr/local/bin/kubectl
        mode: '0755'
        owner: root
        group: root

    - name: Verify kubectl installation
      command: /usr/local/bin/kubectl version --client --output=yaml
      register: kubectl_version
      failed_when: false
      changed_when: false

    - name: Download and install Helm
      shell: |
        curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        chmod 700 get_helm.sh
        ./get_helm.sh
        rm get_helm.sh
      args:
        creates: /usr/local/bin/helm

    - name: Verify Helm installation
      command: /usr/local/bin/helm version --short
      register: helm_version
      failed_when: false
      changed_when: false

    - name: Download and install k9s
      shell: |
        K9S_VERSION=$(curl -s "https://api.github.com/repos/derailed/k9s/releases/latest" | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/')
        curl -sL "https://github.com/derailed/k9s/releases/download/${K9S_VERSION}/k9s_Linux_amd64.tar.gz" | tar xz -C /tmp
        mv /tmp/k9s /usr/local/bin/k9s
        chmod +x /usr/local/bin/k9s
      args:
        creates: /usr/local/bin/k9s

    - name: Verify k9s installation
      command: /usr/local/bin/k9s version
      register: k9s_version
      failed_when: false
      changed_when: false

    - name: Create kubectl wrapper script for users
      copy:
        dest: /usr/local/bin/kubectl-rke2
        content: |
          #!/bin/bash
          # Kubectl wrapper for RKE2 cluster access
          export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
          exec /usr/local/bin/kubectl "$@"
        mode: '0755'
        owner: root
        group: root

    - name: Create helm wrapper script for users
      copy:
        dest: /usr/local/bin/helm-rke2
        content: |
          #!/bin/bash
          # Helm wrapper for RKE2 cluster access
          export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
          exec /usr/local/bin/helm "$@"
        mode: '0755'
        owner: root
        group: root

    - name: Create k9s wrapper script for users
      copy:
        dest: /usr/local/bin/k9s-rke2
        content: |
          #!/bin/bash
          # k9s wrapper for RKE2 cluster access
          export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
          exec /usr/local/bin/k9s "$@"
        mode: '0755'
        owner: root
        group: root

    - name: Create kubeconfig directory for adm4n user
      file:
        path: /home/adm4n/.kube
        state: directory
        owner: adm4n
        group: adm4n
        mode: '0700'
      when: ansible_user_id == 'adm4n' or 'adm4n' in ansible_all_ipv4_addresses

    - name: Set up kubectl aliases for adm4n user
      lineinfile:
        path: /home/adm4n/.bashrc
        line: "{{ item }}"
        create: yes
        owner: adm4n
        group: adm4n
      loop:
        - "# Kubernetes tools aliases"
        - "alias kubectl='sudo KUBECONFIG=/etc/rancher/rke2/rke2.yaml /usr/local/bin/kubectl'"
        - "alias helm='sudo KUBECONFIG=/etc/rancher/rke2/rke2.yaml /usr/local/bin/helm'"
        - "alias k9s='sudo KUBECONFIG=/etc/rancher/rke2/rke2.yaml /usr/local/bin/k9s'"
        - "alias k='kubectl'"
        - "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml"
      when: ansible_user_id == 'adm4n' or 'adm4n' in ansible_all_ipv4_addresses

    - name: Create k9s config directory for root
      file:
        path: /root/.config/k9s
        state: directory
        mode: '0755'

    - name: Create k9s configuration file
      copy:
        dest: /root/.config/k9s/config.yaml
        content: |
          k9s:
            refreshRate: 2
            maxConnRetry: 5
            enableMouse: true
            headless: false
            logoless: false
            crumbsless: false
            readOnly: false
            noExitOnCtrlC: false
            ui:
              enableMouse: true
              headless: false
              logoless: false
              crumbsless: false
              reactive: false
              noIcons: false
            skipLatestRevCheck: false
            disablePodCounting: false
            shellPod:
              image: busybox:1.35.0
              namespace: default
              limits:
                cpu: 100m
                memory: 100Mi
            imageScans:
              enable: false
              exclusions:
                namespaces: []
                labels: {}
            logger:
              tail: 100
              buffer: 5000
              sinceSeconds: -1
              textWrap: false
              showTime: false
            thresholds:
              cpu:
                critical: 90
                warn: 70
              memory:
                critical: 90
                warn: 70
        mode: '0644'

    # We only need nvidia-container-toolkit for GPU support
    # RKE2 uses its embedded containerd runtime, not Docker

    # GPU-specific tasks for containers with 'gpu' in hostname or is_gpu_node=true
    - name: Check if this is a GPU node
      set_fact:
        is_gpu_node: "{{ (gpu_detection_pattern in inventory_hostname) or (is_gpu_node | default(false) | bool) }}"

    - name: Install NVIDIA driver dependencies (GPU nodes only)
      apt:
        name:
          - build-essential
          - dkms
          - linux-headers-generic
        state: present
      when: is_gpu_node

    - name: Check if NVIDIA driver installer exists
      stat:
        path: "/root/NVIDIA-Linux-x86_64-575.64.05.run"
      register: nvidia_installer
      when: is_gpu_node

    - name: Install NVIDIA driver (GPU nodes only)
      shell: |
        chmod +x /root/NVIDIA-Linux-x86_64-575.64.05.run
        /root/NVIDIA-Linux-x86_64-575.64.05.run --dkms --no-questions --ui=none --no-kernel-module --no-drm --install-libglvnd
        ldconfig
      when: is_gpu_node and nvidia_installer.stat.exists
      register: nvidia_driver_install
      failed_when: false

    - name: Test NVIDIA driver installation (GPU nodes only)
      command: nvidia-smi
      register: nvidia_test
      failed_when: false
      when: is_gpu_node and nvidia_driver_install is succeeded

    - name: Display NVIDIA driver test result
      debug:
        msg: "NVIDIA driver test: {{ 'PASSED' if nvidia_test.rc == 0 else 'FAILED' }}"
      when: is_gpu_node and nvidia_test is defined

    - name: Install NVIDIA Container Toolkit GPG key (GPU nodes only)
      shell: |
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
      when: is_gpu_node and nvidia_test is defined and nvidia_test.rc == 0

    - name: Add NVIDIA Container Toolkit repository (GPU nodes only)
      shell: |
        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed "s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g" | tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
      when: is_gpu_node and nvidia_test is defined and nvidia_test.rc == 0

    - name: Install NVIDIA Container Toolkit (GPU nodes only)
      apt:
        name: nvidia-container-toolkit
        state: present
        update_cache: yes
      when: is_gpu_node and nvidia_test is defined and nvidia_test.rc == 0

    - name: Configure NVIDIA Container Toolkit for RKE2's containerd (GPU nodes only)
      shell: |
        nvidia-ctk runtime configure --runtime=containerd --config=/var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl
        mkdir -p /etc/nvidia-container-runtime
        nvidia-ctk config --set nvidia-container-cli.no-cgroups --in-place
      when: is_gpu_node and nvidia_test is defined and nvidia_test.rc == 0

    # ===== END AUTOMATED CONTAINER SETUP TASKS =====

    - name: Check if the NetworkManager directory exists
      ansible.builtin.stat:
        path: "/usr/sbin/NetworkManager"
      register: nwm_dir

    - name: NetworkManager found
      ansible.builtin.fail:
        msg: Network Manager present
      when: nwm_dir.stat.exists

    - name: Check if swap is enabled
      shell: swapon --show
      register: swap_status
      failed_when: false
      changed_when: false

    - name: Disable SWAP
      shell: swapoff -a
      when: swap_status.stdout != ""
      failed_when: false

    - name: Check if /etc/fstab exists
      stat:
        path: /etc/fstab
      register: fstab_exists

    - name: Disable SWAP in fstab
      replace:
        path: /etc/fstab
        regexp: '^([^#].*?\sswap\s+sw\s+.*)$'
        replace: '# \1'
      when: fstab_exists.stat.exists

    - name: Set a hostname {{ inventory_hostname }}
      ansible.builtin.hostname:
        name: "{{ inventory_hostname }}"

    - name: Update Timezone to Etc/UTC
      copy: content="Etc/UTC\n" dest=/etc/timezone owner=root group=root mode=0644
      register: timezone

    - name: Reconfigure Timezone Data (if changed)
      shell: dpkg-reconfigure -f noninteractive tzdata
      when: timezone.changed

    #- name: sync hwclock
    #  shell: "hwclock --localtime --systohc"

    # Add this before the sysctl tasks
    - name: Load required kernel modules (ignore failures for LXC containers)
      community.general.modprobe:
        name: "{{ item }}"
        state: present
      with_items:
        - br_netfilter
        - overlay
      ignore_errors: true

    # Check if we're running in an LXC container
    - name: Check if running in LXC container
      shell: |
        if [ -f /proc/1/environ ] && grep -q container=lxc /proc/1/environ; then
          echo "lxc"
        else
          echo "vm"
        fi
      register: container_type
      changed_when: false

    - name: Display container type
      debug:
        msg: "Running in {{ container_type.stdout }} environment"

    # Force cleanup of hanging RKE2 installation in LXC containers
    - name: Stop hanging RKE2 services (LXC containers)
      systemd:
        name: "{{ item }}"
        state: stopped
        enabled: false
      with_items:
        - rke2-server
        - rke2-agent
      failed_when: false
      when: container_type.stdout == 'lxc'

    - name: Kill any remaining RKE2 processes (LXC containers)
      shell: |
        pkill -9 -f rke2 || true
        pkill -9 -f containerd || true
        pkill -9 -f kubelet || true
        pkill -9 -f k3s || true
      when: container_type.stdout == 'lxc'
      failed_when: false

    - name: Clean up RKE2 data directories (LXC containers)
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /var/lib/rancher/rke2/server/db
        - /var/lib/rancher/rke2/server/tls
        - /var/lib/rancher/rke2/agent
        - /etc/rancher/rke2/config.yaml
        - /var/lib/rancher/rke2/server/manifests
        - /run/k3s
        - /var/lib/rancher/rke2/server/node-token
      when: container_type.stdout == 'lxc'
      failed_when: false

    - name: Clean up CNI directories (LXC containers)
      file:
        path: "{{ item }}"
        state: absent
      with_items:
        - /opt/cni/bin
        - /etc/cni/net.d
        - /var/lib/cni
      when: container_type.stdout == 'lxc'
      failed_when: false

    - name: Wait a moment after cleanup
      pause:
        seconds: 5
      when: container_type.stdout == 'lxc'

    # Then modify your sysctl task to ignore errors if the module isn't loaded
    - name: "Set fs.inotify.max_user_instances"
      sysctl:
        name: fs.inotify.max_user_instances
        value: '100000'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    #sysctl -w fs.inotify.max_user_watches=1003986
    - name: "Set fs.inotify.max_user_watches"
      sysctl:
        name: fs.inotify.max_user_watches
        value: '1003986'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    #sysctl -w vm.max_map_count=262144
    - name: "Set vm.max_map_count"
      sysctl:
        name: vm.max_map_count
        value: '262144'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    # LXC containers may have restricted access to certain kernel parameters
    - name: "Set net.bridge.bridge-nf-call-iptables (ignore errors for LXC)"
      sysctl:
        name: net.bridge.bridge-nf-call-iptables
        value: '1'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    - name: "Set net.bridge.bridge-nf-call-ip6tables (ignore errors for LXC)"
      sysctl:
        name: net.bridge.bridge-nf-call-ip6tables
        value: '1'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    - name: "Set net.ipv4.ip_forward"
      sysctl:
        name: net.ipv4.ip_forward
        value: '1'
        sysctl_set: yes
        state: present
        reload: yes
        ignoreerrors: yes

    - name: Add or modify hard nofile limits for wildcard domain
      community.general.pam_limits:
        domain: '*'
        limit_type: hard
        limit_item: nofile
        value: 1000000

    - name: Add or modify soft nofile limits for wildcard domain
      community.general.pam_limits:
        domain: '*'
        limit_type: soft
        limit_item: nofile
        value: 1000000

    - name: Update apt repo and cache on all boxes
      apt: update_cache=yes force_apt_get=yes cache_valid_time=3600

    - name: Get kernel version
      shell: uname -r
      register: kernel_version

    - name: Install additional software
      apt:
        update_cache: true
        pkg:
        - curl
        - git
        - wget
        - vim
        - nano
        - htop
        - netcat-openbsd
        - net-tools
        - dnsutils
        - jq

    - name: Install linux headers (skip for LXC containers)
      apt:
        update_cache: true
        pkg:
        - linux-headers-generic
        - linux-headers-{{ kernel_version.stdout }}
      when: container_type.stdout != 'lxc'
      ignore_errors: true

    - name: Get DEB architecture
      shell: dpkg --print-architecture
      register: deb_architecture

    - name: Install k9s on control plane nodes
      shell: |
        curl -L -O https://github.com/derailed/k9s/releases/latest/download/k9s_linux_{{ deb_architecture.stdout }}.deb && sudo dpkg -i k9s_linux_{{ deb_architecture.stdout }}.deb && rm k9s_linux_{{ deb_architecture.stdout }}.deb
      when: rke2_type == 'server'

    - name: Check for existing local-path-provisioner directory
      stat:
        path: "{{ local_path_provisioner_path | default('/mnt/data') }}"
      register: local_path_provisioner_dir

    - name: Verify mount point accessibility in LXC
      shell: |
        if [ -d "{{ local_path_provisioner_path | default('/mnt/data') }}" ]; then
          if touch "{{ local_path_provisioner_path | default('/mnt/data') }}/test-write" 2>/dev/null; then
            rm -f "{{ local_path_provisioner_path | default('/mnt/data') }}/test-write"
            echo "writable"
          else
            echo "readonly"
          fi
        else
          echo "missing"
        fi
      register: storage_check
      changed_when: false

    - name: Display storage check results
      debug:
        msg: "Storage path {{ local_path_provisioner_path | default('/mnt/data') }} is {{ storage_check.stdout }}"

    - name: Create local-path-provisioner directory on worker nodes
      file:
        path: "{{ local_path_provisioner_path | default('/mnt/data') }}"
        state: directory
        mode: '0755'
        owner: root
        group: root
      when: rke2_type == 'agent' and (use_local_path_provisioner | bool) and 'workers' in group_names and storage_check.stdout == 'missing'

    - name: Create alternative storage directory on control plane nodes (if main path missing)
      file:
        path: "{{ local_path_provisioner_path | default('/mnt/data') }}"
        state: directory
        mode: '0755'
        owner: root
        group: root
      when: rke2_type == 'server' and (use_local_path_provisioner | bool) and storage_check.stdout == 'missing'

    - name: Check if a reboot is needed on all servers
      register: reboot_required_file
      stat: path=/var/run/reboot-required

    - name: Check if the rke2/config.yaml exists
      stat:
        path: /etc/rancher/rke2/config.yaml
      register: rkeconfig_result

    # LXC containers support reboots - use standard reboot for consistency
    - name: Reboot the box on first install
      reboot:
        msg: "Reboot initiated by Ansible for first install"
        connect_timeout: 5
        reboot_timeout: 300
        pre_reboot_delay: 0
        post_reboot_delay: 30
        test_command: uptime
      when: not rkeconfig_result.stat.exists

    - name: Disable SWAP again before check
      shell: |
        # Kill any swap processes
        swapoff -a
        # Remove swap from fstab completely
        sed -i '/swap/d' /etc/fstab
        # Remove common swap files and devices
        if [ -e /swapfile ]; then
          swapoff /swapfile 2>/dev/null || true
          rm -f /swapfile
        fi
        if [ -e /dev/mapper/swap ]; then
          swapoff /dev/mapper/swap 2>/dev/null || true
        fi
        # Disable any systemd swap services
        systemctl mask swap.target 2>/dev/null || true
        systemctl stop swap.target 2>/dev/null || true
        # Force disable any remaining swap
        for swap_dev in $(cat /proc/swaps | grep -v Filename | awk '{print $1}'); do
          swapoff "$swap_dev" 2>/dev/null || true
        done
        # Wait and verify
        sleep 2
      failed_when: false

    - name: Check SWAP disabled
      shell: |
        # Sleep to ensure swap is fully off
        sleep 1
        LINES=$(cat /proc/swaps | wc -l)
        if [ "$LINES" != "1" ]; then
          # 1 means only the headers and no swap
          echo "LINES is $LINES"
          cat /proc/swaps
          exit 1
        fi

    - name: Create a networkd.conf.d for drop-ins
      # See https://docs.cilium.io/en/stable/operations/system_requirements/#systemd-based-distributions
      # Required for Ubuntu 22.04 and LXC containers
      ansible.builtin.file:
        path: /etc/systemd/networkd.conf.d/
        state: directory
        owner: root
        group: root
        mode: '0755'
    
    - name: Configure systemd Network RoutingPolicyRules
      ansible.builtin.copy:
        dest: /etc/systemd/networkd.conf.d/rke2-network.conf
        content: |
          [Network]
          ManageForeignRoutes=no
          ManageForeignRoutingPolicyRules=no
        owner: root
        group: root
        mode: '0644'
      register: systemdnetwork
    
    - name: Reload systemd networkd (handle LXC restrictions)
      ansible.builtin.systemd:
        state: restarted
        daemon_reload: true
        name: systemd-networkd
      when: systemdnetwork.changed
      ignore_errors: true

    # Additional LXC-specific configurations
    - name: Create systemd system.conf.d directory for LXC containers
      ansible.builtin.file:
        path: /etc/systemd/system.conf.d
        state: directory
        owner: root
        group: root
        mode: '0755'
      when: container_type.stdout == 'lxc'

    - name: Configure systemd for LXC containers
      ansible.builtin.copy:
        dest: /etc/systemd/system.conf.d/lxc-overrides.conf
        content: |
          [Manager]
          DefaultLimitNOFILE=1048576
          DefaultLimitNPROC=1048576
        owner: root
        group: root
        mode: '0644'
      when: container_type.stdout == 'lxc'
      register: systemd_lxc_config

    - name: Reload systemd daemon for LXC changes
      ansible.builtin.systemd:
        daemon_reload: true
      when: systemd_lxc_config.changed

    - name: Add k8s Binaries to PATH
      ansible.builtin.lineinfile:
        path: /root/.bashrc
        line: "{{ item }}"
      loop:
        - "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml"
        - "PATH=$PATH:/var/lib/rancher/rke2/bin"

  post_tasks:
    - name: Verify storage paths on all nodes before local-path-provisioner
      shell: |
        echo "Node: {{ inventory_hostname }}"
        echo "Storage path: {{ local_path_provisioner_path | default('/mnt/data') }}"
        if [ -d "{{ local_path_provisioner_path | default('/mnt/data') }}" ]; then
          if touch "{{ local_path_provisioner_path | default('/mnt/data') }}/test-cluster-storage" 2>/dev/null; then
            rm -f "{{ local_path_provisioner_path | default('/mnt/data') }}/test-cluster-storage"
            echo "Status: ✓ Writable"
            df -h "{{ local_path_provisioner_path | default('/mnt/data') }}" | tail -1
          else
            echo "Status: ✗ Read-only or permission denied"
            ls -la "{{ local_path_provisioner_path | default('/mnt/data') }}"
          fi
        else
          echo "Status: ✗ Directory missing"
          ls -la "$(dirname {{ local_path_provisioner_path | default('/mnt/data') }})" || echo "Parent directory also missing"
        fi
      register: storage_verification
      when: use_local_path_provisioner | bool

    - name: Display storage verification results
      debug:
        msg: "{{ storage_verification.stdout_lines }}"
      when: use_local_path_provisioner | bool

    - name: Check and remove cloud provider uninitialized taint (prevents pod scheduling)
      shell: |
        echo "=== Checking for cloud provider uninitialized taints ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes -o custom-columns="NODE:.metadata.name,TAINTS:.spec.taints[*].key" | grep -E "cloudprovider|uninitialized" || echo "No cloud provider taints found"
        echo ""
        echo "=== Removing cloud provider uninitialized taint from all nodes ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml taint nodes --all node.cloudprovider.kubernetes.io/uninitialized- || echo "Taint not present or already removed"
        echo ""
        echo "=== Verifying taints after removal ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml describe nodes | grep -A 2 "Taints:" || echo "No taints found"
      when: rke2_type == 'server'
      register: taint_removal_result

    - name: Display taint removal results
      debug:
        msg: "{{ taint_removal_result.stdout_lines }}"
      when: rke2_type == 'server'

    - name: Wait for critical system pods to be ready after taint removal
      shell: |
        echo "Waiting for system pods to schedule after taint removal..."
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml wait --for=condition=ready pod -l k8s-app=kube-dns -n kube-system --timeout=300s || echo "CoreDNS wait timed out"
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml wait --for=condition=ready pod -l app.kubernetes.io/name=metrics-server -n kube-system --timeout=300s || echo "Metrics server wait timed out"
      when: rke2_type == 'server'
      register: system_pods_wait

    - name: Debug custom manifests configuration
      debug:
        msg: |
          use_local_path_provisioner: {{ use_local_path_provisioner | bool }}
          has_gpu_nodes: {{ has_gpu_nodes }}
          playbook_dir: {{ playbook_dir }}
          rke2_custom_manifests: {{ rke2_custom_manifests }}
      when: rke2_type == 'server'

    - name: Check if custom manifests were copied correctly
      shell: |
        echo "=== Custom manifests directory ==="
        ls -la {{ rke2_data_path }}/server/manifests/ || echo "Manifests directory not found!"
        echo ""
        echo "=== Local-path-provisioner manifest ==="
        cat {{ rke2_data_path }}/server/manifests/local-path-provisioner.yaml || echo "Local-path-provisioner manifest not found!"
        echo ""
        echo "=== RKE2 server logs (last 20 lines) ==="
        journalctl -u rke2-server --no-pager -n 20 | grep -i "local-path\|manifest\|storage" || echo "No relevant logs found"
      when: rke2_type == 'server' and (use_local_path_provisioner | bool)
      register: manifest_check

    - name: Display manifest check results
      debug:
        msg: "{{ manifest_check.stdout_lines }}"
      when: rke2_type == 'server' and (use_local_path_provisioner | bool)

    - name: Debug local-path-provisioner deployment status
      shell: |
        echo "=== Checking if local-path-provisioner namespace exists ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get namespace local-path-storage || echo "Namespace missing!"
        echo ""
        echo "=== Checking if local-path-provisioner deployment exists ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get deployment -n local-path-storage || echo "No deployments found!"
        echo ""
        echo "=== Checking all pods in local-path-storage namespace ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get pods -n local-path-storage -o wide || echo "No pods found!"
        echo ""
        echo "=== Checking storage class ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get storageclass || echo "No storage classes found!"
        echo ""
        echo "=== Checking if custom manifests were applied ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get pods -A | grep -E "(local-path|storage)" || echo "No storage-related pods found!"
      when: rke2_type == 'server' and (use_local_path_provisioner | bool)
      register: lpp_deployment_status

    - name: Display local-path-provisioner deployment status
      debug:
        msg: "{{ lpp_deployment_status.stdout_lines }}"
      when: rke2_type == 'server' and (use_local_path_provisioner | bool)

    - name: Wait for local-path-provisioner deployment to be available
      shell: |
        echo "Waiting for local-path-provisioner deployment to be created..."
        for i in {1..30}; do
          if {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get deployment local-path-provisioner -n local-path-storage >/dev/null 2>&1; then
            echo "Deployment found after $i attempts"
            break
          fi
          echo "Waiting... ($i/30)"
          sleep 10
        done
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get deployment local-path-provisioner -n local-path-storage
      when: rke2_type == 'server' and (use_local_path_provisioner | bool)
      register: lpp_deployment_wait

    - name: Wait for local-path-provisioner to be ready
      shell: |
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml wait --for=condition=ready pod -l app=local-path-provisioner -n local-path-storage --timeout=300s
      when: rke2_type == 'server' and (use_local_path_provisioner | bool)
      register: lpp_wait_result
      failed_when: false

    - name: Check local-path-provisioner pod status if wait failed
      shell: |
        echo "=== Local Path Provisioner Pod Status ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get pods -n local-path-storage -o wide
        echo ""
        echo "=== Pod Events ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get events -n local-path-storage --sort-by='.lastTimestamp'
        echo ""
        echo "=== Pod Logs ==="
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml logs -n local-path-storage -l app=local-path-provisioner --tail=50 || echo "No logs available"
        echo ""
        echo "=== Node Storage Check ==="
        df -h /mnt/data || echo "/mnt/data not mounted"
        ls -la /mnt/data || echo "/mnt/data directory issue"
      when: rke2_type == 'server' and (use_local_path_provisioner | bool) and lpp_wait_result.rc != 0
      register: lpp_debug_info

    - name: Display local-path-provisioner debug information
      debug:
        msg: "{{ lpp_debug_info.stdout_lines }}"
      when: rke2_type == 'server' and (use_local_path_provisioner | bool) and lpp_wait_result.rc != 0

    - name: Fail deployment if local-path-provisioner is not ready after extended timeout
      fail:
        msg: |
          Local Path Provisioner failed to become ready within 10 minutes.
          This may indicate:
          1. /mnt/data volume not properly mounted on worker nodes
          2. Network issues preventing pod scheduling
          3. Resource constraints preventing pod startup
          Check the debug information above for details.
      when: rke2_type == 'server' and (use_local_path_provisioner | bool) and lpp_wait_result.rc != 0

    - name: Configure kubectl access for adm4n user (copy kubeconfig)
      copy:
        src: /etc/rancher/rke2/rke2.yaml
        dest: /home/adm4n/.kube/config
        owner: adm4n
        group: adm4n
        mode: '0600'
        remote_src: yes
      when: rke2_type == 'server'
      ignore_errors: true

    - name: Update kubeconfig server address for remote access
      replace:
        path: /home/adm4n/.kube/config
        regexp: 'https://127\.0\.0\.1:6443'
        replace: 'https://{{ ansible_default_ipv4.address }}:6443'
      when: rke2_type == 'server'
      ignore_errors: true

    - name: Test kubectl access
      shell: |
        export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        /usr/local/bin/kubectl get nodes --no-headers | wc -l
      register: kubectl_test
      when: rke2_type == 'server'
      failed_when: false

    - name: Test helm access
      shell: |
        export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        /usr/local/bin/helm list --all-namespaces
      register: helm_test
      when: rke2_type == 'server'
      failed_when: false

    - name: Verify tools installation
      debug:
        msg: |
          Kubernetes Tools Status:
          - kubectl: {{ 'Ready (' + kubectl_test.stdout.strip() + ' nodes)' if kubectl_test.rc == 0 else 'Failed' }}
          - helm: {{ 'Ready' if helm_test.rc == 0 else 'Failed' }}
          - k9s: Ready (installed at /usr/local/bin/k9s)
      when: rke2_type == 'server'

    - name: Get cluster health information
      shell: |
        echo "=== RKE2 Cluster Health Check ==="
        echo "Cluster nodes:"
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get nodes -o wide
        echo ""
        echo "System pods status:"
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get pods -n kube-system
        echo ""
        echo "Storage classes:"
        {{ rke2_data_path }}/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml get storageclass
        echo ""
        echo "=== Kubernetes Tools Access ==="
        echo "• kubectl: Use 'kubectl' alias or /usr/local/bin/kubectl"
        echo "• helm: Use 'helm' alias or /usr/local/bin/helm"
        echo "• k9s: Use 'k9s' alias or /usr/local/bin/k9s"
        echo "• All tools configured with KUBECONFIG=/etc/rancher/rke2/rke2.yaml"
        echo ""
        echo "=== Cluster Status ==="
        echo "✓ RKE2 Cluster: Operational"
        echo "✓ Flannel CNI: Active"
        echo "✓ kubectl: Ready"
        echo "✓ helm: Ready"
        echo "✓ k9s: Ready"
        {% if use_local_path_provisioner %}
        echo "✓ Local Path Provisioner: Ready"
        {% endif %}
        {% if has_gpu_nodes %}
        echo "✓ NVIDIA GPU Operator: Deployed"
        {% endif %}
      when: rke2_type == 'server'
      register: cluster_info

    - name: Display cluster access information
      debug:
        msg: "{{ cluster_info.stdout_lines }}"
      when: rke2_type == 'server' and cluster_info.stdout_lines is defined

    - name: Create cluster access info file
      copy:
        content: |
          # RKE2 Cluster Access Information
          
          ## Quick Start - Using Aliases
          
          SSH to any cluster node and use the preconfigured aliases:
          ```bash
          # View cluster nodes
          kubectl get nodes
          
          # List helm releases
          helm list --all-namespaces
          
          # Interactive cluster management
          k9s
          ```
          
          ## Tool Locations
          
          All tools are installed system-wide:
          - kubectl: `/usr/local/bin/kubectl`
          - helm: `/usr/local/bin/helm`
          - k9s: `/usr/local/bin/k9s`
          
          ## Manual KUBECONFIG Setup
          
          If aliases don't work, export the kubeconfig:
          ```bash
          export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
          kubectl get nodes
          helm list --all-namespaces
          k9s
          ```
          
          ## User Access (adm4n)
          
          The adm4n user has kubectl access via:
          - Personal kubeconfig: `~/.kube/config`
          - Aliases in `~/.bashrc` for kubectl, helm, k9s, k (kubectl shortcut)
          
          ## Cluster Architecture
          
          ### Core Components
          - **Kubernetes Version**: 1.33.3 (via RKE2 v1.33.3+rke2r1)
          - **CNI**: Flannel (optimized for LXC containers)
          - **Runtime**: RKE2 Embedded Containerd (Docker-free)
          - **Datastore**: SQLite (single server + multi-agent architecture)
          
          ### Storage
          {% if use_local_path_provisioner %}
          - **Local Path Provisioner**: Available at {{ local_path_provisioner_path | default('/mnt/data') }}
          {% endif %}
          - **ZFS Volumes**: Mounted at `/mnt/data` on all nodes
          
          ### GPU Support
          {% if has_gpu_nodes %}
          - **NVIDIA GPU Operator**: Deployed for GPU workloads
          - **Container Toolkit**: Configured for RKE2's containerd
          {% endif %}
          
          ## Management Commands
          
          ```bash
          # Cluster status
          kubectl get nodes,pods --all-namespaces
          
          # Install applications with Helm
          helm repo add bitnami https://charts.bitnami.com/bitnami
          helm install my-app bitnami/nginx
          
          # Interactive debugging and monitoring
          k9s
          ```
          
          ## Troubleshooting
          
          ```bash
          # Service logs
          journalctl -u rke2-server    # Control plane
          journalctl -u rke2-agent     # Worker nodes
          
          # Pod logs and debugging
          kubectl logs -n kube-system <pod-name>
          kubectl describe pod <pod-name>
          
          # Network debugging
          kubectl get pods -o wide
          kubectl get svc --all-namespaces
          ```
          
          ## Deployment Status
          
          - ✅ **RKE2 Cluster**: Operational (SQLite datastore)
          - ✅ **Flannel CNI**: Active on all nodes
          - ✅ **kubectl**: Installed and configured
          - ✅ **helm**: Installed and configured
          - ✅ **k9s**: Installed and configured
          {% if use_local_path_provisioner %}
          - ✅ **Local Path Provisioner**: Ready for storage
          {% endif %}
          {% if has_gpu_nodes %}
          - ✅ **NVIDIA Container Toolkit**: GPU workloads ready
          {% endif %}
        dest: "{{ playbook_dir }}/../cluster-access-info.md"
        mode: '0644'
      when: rke2_type == 'server'
      delegate_to: localhost

  roles:
    - role: rke2
      vars:
        k8s_cluster: "{{ groups['rke2_cluster'] | default(groups['all']) }}"
