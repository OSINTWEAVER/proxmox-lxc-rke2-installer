apiVersion: v1
kind: Pod
metadata:
  name: nvidia-runtime-test
spec:
  runtimeClassName: nvidia
  restartPolicy: Never
  nodeSelector:
    node-type: gpu-worker
  containers:
  - name: test
    image: nvidia/cuda:12.9.1-base-ubuntu24.04
    command: ["/bin/bash"]
    args:
    - -c
    - |
      echo "🚀 NVIDIA GPU Test on Node: $(hostname)"
      echo "========================================"
      echo "Runtime: nvidia"
      echo "Image: nvidia/cuda:12.9.1-base-ubuntu24.04"
      echo ""
      
      echo "📋 NVIDIA System Information:"
      echo "-----------------------------"
      if command -v nvidia-smi >/dev/null 2>&1; then
        echo "✅ nvidia-smi found - running GPU detection..."
        nvidia-smi
        echo ""
        echo "🎯 GPU Details:"
        nvidia-smi --query-gpu=name,driver_version,memory.total,memory.used,memory.free --format=csv
      else
        echo "❌ nvidia-smi not available"
        echo "   This could mean:"
        echo "   - GPU not passed through to LXC container"
        echo "   - NVIDIA drivers not installed on host"
        echo "   - GPU devices not accessible"
      fi
      
      echo ""
      echo "🔍 Device Information:"
      echo "---------------------"
      echo "NVIDIA devices in /dev:"
      ls -la /dev/nvidia* 2>/dev/null || echo "   No /dev/nvidia* devices found"
      
      echo ""
      echo "🏁 Test completed on node: $(hostname)"
      sleep 15
